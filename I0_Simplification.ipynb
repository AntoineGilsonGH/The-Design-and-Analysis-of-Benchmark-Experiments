{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import uniform, norm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_antoine = \"/Users/antoine.gilson/Desktop/The-Design-and-Analysis-of-Benchmark-Experiments/Plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2_values = np.linspace(0, 0.16, 9)\n",
    "M = [150,200,500,1000,2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction, variables, training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_test_sets(n=150, m=150, seed=42):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    \n",
    "    epsilon = np.random.normal(0, 1, n + m)\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "    \n",
    "    # Création de la figure avec subplots\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plotting X Training set\n",
    "    ax[0, 0].scatter(np.arange(n), x_train, label=\"X Training set n = 150\", color=\"blue\", s=10)\n",
    "    ax[0, 0].set_title(\"Training Set X\")\n",
    "    ax[0, 0].set_xlabel(\"Index\")\n",
    "    ax[0, 0].set_ylabel(\"Value\")\n",
    "    ax[0, 0].legend()\n",
    "    ax[0, 0].grid()\n",
    "\n",
    "    # Plotting X Test set\n",
    "    ax[0, 1].scatter(np.arange(n, n + m), x_test, label=\"X Test set m = 150\", color=\"red\", s=10)\n",
    "    ax[0, 1].set_title(\"Test Set X\")\n",
    "    ax[0, 1].set_xlabel(\"Index\")\n",
    "    ax[0, 1].set_ylabel(\"Value\")\n",
    "    ax[0, 1].legend()\n",
    "    ax[0, 1].grid()\n",
    "\n",
    "    # Plotting Epsilon Training set\n",
    "    ax[1, 0].scatter(np.arange(n), epsilon_train, label=\"Epsilon Training set n = 150\", color=\"green\", s=10)\n",
    "    ax[1, 0].set_title(\"Training Set Epsilon\")\n",
    "    ax[1, 0].set_xlabel(\"Index\")\n",
    "    ax[1, 0].set_ylabel(\"Value\")\n",
    "    ax[1, 0].legend()\n",
    "    ax[1, 0].grid()\n",
    "\n",
    "    # Plotting Epsilon Test set\n",
    "    ax[1, 1].scatter(np.arange(n, n + m), epsilon_test, label=\"Epsilon Test set m = 150\", color=\"orange\", s=10)\n",
    "    ax[1, 1].set_title(\"Test Set Epsilon\")\n",
    "    ax[1, 1].set_xlabel(\"Index\")\n",
    "    ax[1, 1].set_ylabel(\"Value\")\n",
    "    ax[1, 1].legend()\n",
    "    ax[1, 1].grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(n), x_train, label = \"X Training set n = 150\", color = \"blue\", s = 8)\n",
    "    plt.scatter(np.arange(n, n+m), x_test, label = \"X Test set m = 150\", color = \"red\", s = 8)\n",
    "    plt.xlabel(\"Point de l'ensemble de test ou d'entrainement\")\n",
    "    plt.ylabel(\"Valeur de X\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(n), epsilon_train, label = \"Eps Training set n = 150\", color = \"green\", s = 8)\n",
    "    plt.scatter(np.arange(n, n+m), epsilon_test, label = \"Eps Test set m = 150\", color = \"orange\", s = 8)\n",
    "    plt.xlabel(\"Point de l'ensemble de test ou d'entrainement\")\n",
    "    plt.ylabel(\"Valeur de Epsilon\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_test_sets_hist(n=150, m=2000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    \n",
    "    epsilon = np.random.normal(0, 1, n + m)\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    x_vals = np.linspace(0, 5, 100)\n",
    "    epsilon_vals = np.linspace(-4, 4, 100)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(x_train, bins=30, label=\"X Training set n_train = 150\", color=\"blue\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.hist(x_test, bins=30, label=\"X Test set n_test = 2000\", color=\"red\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.plot(x_vals, uniform.pdf(x_vals, 0, 5), 'k-', label=\"Uniform(0, 5)\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(epsilon_train, bins=30, label=\"Eps Training set n_train = 150\", color=\"green\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.hist(epsilon_test, bins=30, label=\"Eps Test set m_train = 2000\", color=\"orange\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.plot(epsilon_vals, norm.pdf(epsilon_vals, 0, 1), 'k-', label=\"Normal(0, 1)\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical approach of model precision and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances_metrics_boxplot():\n",
    "    np.random.seed(42)\n",
    "    n = 150\n",
    "    M = [150, 200, 500, 1000, 2000]\n",
    "    beta1 = 2\n",
    "\n",
    "\n",
    "    for m in M:\n",
    "\n",
    "        x = np.random.uniform(0, 5, n + m)\n",
    "        x_train = x[:n]\n",
    "\n",
    "        epsilon = np.random.normal(0, 1, n + m)\n",
    "        epsilon_train = epsilon[:n]\n",
    "        x_test = x[n:n + m]\n",
    "        epsilon_test = epsilon[n:n + m]\n",
    "\n",
    "        for beta2 in [0.3, 0.5,1]:\n",
    "            y = beta1 * x + beta2 * x**2 + epsilon\n",
    "            y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "            y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "            # Linear Model : A\n",
    "            model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "            y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "            pa = (y_test - y_pred_a)**2\n",
    "\n",
    "            # Quadratic Model : B\n",
    "            x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "            x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "            model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "            y_pred_b = model_b.predict(x_test_quad)\n",
    "            pb = (y_test - y_pred_b)**2\n",
    "\n",
    "            differences = pa - pb\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f\"Erreur de prédiction quadratique pb = (y-yb)**2 pour beta2 = {beta2} et pour m = {m}\")\n",
    "\n",
    "            # Scatter plot des différences\n",
    "            sns.scatterplot(x=x_test, y=pb, color=\"red\", s=6, label=f\"Differences pour beta2 = {beta2} et pour m = {m}\")\n",
    "            plt.axhline(y=0, color='black', linestyle='--', label='y = 0')\n",
    "\n",
    "            # Ajouter les données pour le boxplot\n",
    "            box = plt.boxplot(pb, positions=[max(x_test) + 1], widths=0.5, patch_artist=True,\n",
    "                              boxprops=dict(facecolor='blue', color='blue', alpha=0.5),\n",
    "                              medianprops=dict(color='yellow'),\n",
    "                              whiskerprops=dict(color='blue'),\n",
    "                              capprops=dict(color='blue'),\n",
    "                              flierprops=dict(color='blue', markeredgecolor='blue'))\n",
    "\n",
    "            # Récupérer les valeurs des whiskers et des quartiles\n",
    "            whisker_min = box['whiskers'][0].get_ydata()[1]\n",
    "            whisker_max = box['whiskers'][1].get_ydata()[1]\n",
    "            q1 = box['boxes'][0].get_path().vertices[0, 1]\n",
    "            median = box['medians'][0].get_ydata()[0]\n",
    "            q3 = box['boxes'][0].get_path().vertices[2, 1]\n",
    "\n",
    "            # Annoter les valeurs sur le graphique\n",
    "            plt.text(max(x_test) + 1, whisker_min, f'Whisker Min: {whisker_min:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, whisker_max, f'Whisker Max: {whisker_max:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q1, f'Q1: {q1:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, median, f'Median: {median:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q3, f'Q3: {q3:.2f}', horizontalalignment='center', color='black')\n",
    "\n",
    "            plt.xlabel(\"X test set\")\n",
    "            plt.ylabel(\"Differences\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "\n",
    "            filename = f\"{path_antoine}/Erreurs/ModeleB/CondToX/plot_m_{m}_beta2_{beta2}.png\"\n",
    "            plt.savefig(filename)\n",
    "\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics_hist():\n",
    "    \n",
    "    n = 150\n",
    "\n",
    "    M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "    for m in M:\n",
    "\n",
    "      np.random.seed(42)\n",
    "      beta1 = 2\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+m]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+m]\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "\n",
    "          coef_a = model_a.coef_[0]\n",
    "          intercept_a = model_a.intercept_     \n",
    "        \n",
    "          # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          coef_b = model_b.coef_\n",
    "          intercept_b = model_b.intercept_\n",
    "\n",
    "          differences = pa - pb\n",
    "\n",
    "          plt.figure() # Erreur individuelle des modèles\n",
    "          plt.title(\"Erreurs de prédiction du modèle linéaire pour \" f\"beta2 = {beta2}\" \" et pour \"f\"m = {m}\" )  \n",
    "          plt.hist(pa, bins=30, label=f\"pa = (y - ya)^2 pour beta2 = {beta2} et pour m = {m}\", color=\"red\", edgecolor='black', alpha=0.7)\n",
    "          plt.xlabel(\" Erreur associée \")\n",
    "          plt.ylabel(\"Nombre d'occurences\")      \n",
    "          plt.grid()\n",
    "          plt.legend()\n",
    "          filename = f\"{path_antoine}/Erreurs/ModeleA/Frequency/plot_m_{m}_beta2_{beta2}.png\"\n",
    "          plt.savefig(filename)\n",
    "          plt.show()\n",
    "\n",
    "          plt.figure()\n",
    "          plt.title(\"Erreurs de prédiction du modèle quadratique pour \" f\"beta2 = {beta2}\" \" et pour \"f\"m = {m}\" )        \n",
    "          plt.hist(pb, bins=30, label=f\"pb = (y - yb)^2 pour beta2 = {beta2} et pour m = {m}\", color=\"orange\", edgecolor='black', alpha=0.7)\n",
    "          plt.xlabel(\" Erreur associée \")\n",
    "          plt.ylabel(\"Nombre d'occurences\")      \n",
    "          plt.grid()\n",
    "          plt.legend()\n",
    "          filename = f\"{path_antoine}/Erreurs/ModeleB/Frequency/plot_m_{m}_beta2_{beta2}.png\"\n",
    "          plt.savefig(filename)\n",
    "          plt.show()\n",
    "\n",
    "          plt.figure()\n",
    "          plt.title(\"Différence de prédiction des modèles \" f\"beta2 = {beta2}\" \" et pour \"f\"m = {m}\" )        \n",
    "          plt.hist(differences, bins=30, label=f\"pa - pb pour beta2 = {beta2} et pour m = {m}\", color=\"blue\", edgecolor='black', alpha=0.7)\n",
    "          plt.xlabel(\" Erreur associée \")\n",
    "          plt.ylabel(\"Nombre d'occurences\")      \n",
    "          plt.grid()\n",
    "          plt.legend()\n",
    "          filename = f\"{path_antoine}/Erreurs/Differences/Frequency/plot_m_{m}_beta2_{beta2}.png\"\n",
    "          plt.savefig(filename)\n",
    "          plt.show()\n",
    "\n",
    "          y = beta1*x + beta2*x**2 +epsilon \n",
    "          eq_a = intercept_a + coef_a * x\n",
    "          eq_a_train = intercept_a + coef_a * x_train\n",
    "          eq_a_test = intercept_a + coef_a * x_test\n",
    "          eq_b = intercept_b + coef_b[0] * x + coef_b[1] * x**2\n",
    "          eq_b_train = intercept_b + coef_b[0] * x_train + coef_b[1] * x_train**2\n",
    "          eq_b_test = intercept_b + coef_b[0] * x_test + coef_b[1] * x_test**2\n",
    "\n",
    "          plt.figure() # Fits des modèles \n",
    "          plt.title(\"Fits d'entrainement du modèle quadratique pour \" f\"beta2 = {beta2}\" \" et pour \"f\"m = {m}\" )  \n",
    "          plt.plot(x[np.argsort(x)], y[np.argsort(x)], label = \"Y réel\", color = \"green\")\n",
    "          plt.plot(x[np.argsort(x)], eq_a[np.argsort(x)], label = \"Prédiction linéaire\", color = \"Red\")\n",
    "          plt.plot(x[np.argsort(x)], eq_b[np.argsort(x)], label = \"Prédiction quadratique\", color = \"Orange\")\n",
    "          plt.xlabel(\"X\")\n",
    "          plt.ylabel(\"Y\")\n",
    "          plt.grid()\n",
    "          plt.legend()  \n",
    "          filename = f\"{path_antoine}/Fits/Continuous/plot_m_{m}_beta2_{beta2}.png\"\n",
    "          plt.savefig(filename)\n",
    "          plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE():\n",
    "    \n",
    "    n = 150\n",
    "    m = 2000\n",
    "\n",
    "    np.random.seed(42)\n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    mse_A = []\n",
    "    mse_B = []\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "            \n",
    "        y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "        y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "         # Linear Model : A\n",
    "        model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "        y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "\n",
    "        mse_A.append(mean_squared_error(y_pred_a, y_test))\n",
    "    \n",
    "          # Quadratic Model : B\n",
    "        x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "        x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "        model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "        y_pred_b = model_b.predict(x_test_quad)\n",
    "        mse_B.append(mean_squared_error(y_pred_b, y_test))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Différence des MSE\")\n",
    "    plt.plot(beta2_values, mse_A, label = \"MSE_A\")\n",
    "    plt.plot(beta2_values, mse_B, label = \"MSE_B\")\n",
    "       \n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics():\n",
    "    \n",
    "    n = 150\n",
    "\n",
    "    M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "    empirical_means_A = []\n",
    "    standard_deviations_A = []\n",
    "    medians_A = []\n",
    "    Q1_A = []\n",
    "    Q3_A = []\n",
    "    IQR_A = []\n",
    "\n",
    "    empirical_means_B = []\n",
    "    standard_deviations_B = []\n",
    "    medians_B = []\n",
    "    Q1_B = []\n",
    "    Q3_B = []\n",
    "    IQR_B = []\n",
    "\n",
    "    for m in M:\n",
    "\n",
    "      a = []\n",
    "      b = []\n",
    "      c = []\n",
    "      d = []\n",
    "      q = []\n",
    "      iqr = []\n",
    "\n",
    "      e = []\n",
    "      f = []\n",
    "      g = []\n",
    "      h = []\n",
    "      q_ = []\n",
    "      iqr_ = []\n",
    "\n",
    "\n",
    "      np.random.seed(42)\n",
    "      beta1 = 2\n",
    "      beta2_values_bis = [0.2, 0.3, 0.4, 0.5, 1]\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+m]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+m]\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "        # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "        # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          mean_a = sum(pa)/m\n",
    "          mean_b = sum(pb)/m\n",
    "\n",
    "          a.append(mean_a)\n",
    "          e.append(mean_b)\n",
    "\n",
    "          std_a = np.std(pa, ddof=1)\n",
    "          std_b = np.std(pb, ddof = 1)\n",
    "\n",
    "          b.append(std_a)\n",
    "          f.append(std_b)\n",
    "\n",
    "          q1_a = np.percentile(pa, 25)\n",
    "          med_a = np.percentile(pa, 50)        \n",
    "          q3_a = np.percentile(pa, 75)\n",
    "\n",
    "          q1_b = np.percentile(pb, 25)\n",
    "          med_b = np.percentile(pb, 50)        \n",
    "          q3_b = np.percentile(pb, 75)\n",
    "\n",
    "          c.append(med_a)\n",
    "          g.append(med_b)\n",
    "          d.append(q1_a)\n",
    "          q.append(q3_a)\n",
    "          h.append(q1_b)\n",
    "          q_.append(q3_b)\n",
    "\n",
    "          iqr.append(q3_a - q1_a)\n",
    "          iqr_.append(q3_b - q1_b)\n",
    "\n",
    "      empirical_means_A.append(a)\n",
    "      empirical_means_B.append(e)\n",
    "      standard_deviations_A.append(b)\n",
    "      standard_deviations_B.append(f)\n",
    "      medians_A.append(c)\n",
    "      medians_B.append(g)\n",
    "      Q1_A.append(d)\n",
    "      Q1_B.append(h)\n",
    "      Q3_A.append(q)\n",
    "      Q3_B.append(q_)\n",
    "      IQR_A.append(iqr)\n",
    "      IQR_B.append(iqr_)\n",
    "\n",
    "    return empirical_means_A, standard_deviations_A, medians_A, Q1_A, Q3_A, IQR_A, empirical_means_B, standard_deviations_B, medians_B, Q1_B, Q3_B, IQR_B\n",
    "\n",
    "\n",
    "empirical_means_A, standard_deviations_A, medians_A, Q1_A, Q3_A, IQR_A, empirical_means_B, standard_deviations_B, medians_B, Q1_B, Q3_B, IQR_B = descriptive_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    \n",
    "    M = [150, 200, 500, 1000, 2000]\n",
    "    \n",
    "    empirical_means_A, standard_deviations_A, medians_A, Q1_A, Q3_A, IQR_A, empirical_means_B, standard_deviations_B, medians_B, Q1_B, Q3_B, IQR_B = descriptive_statistics()\n",
    "    \n",
    "    \n",
    "    data_A_all = {\n",
    "    \"Empirical Mean\": empirical_means_A,\n",
    "    \"Standard Deviation\": standard_deviations_A,\n",
    "    \"Median\": medians_A,\n",
    "    \"Q1\": Q1_A,\n",
    "    \"Q3\": Q3_A,\n",
    "    \"IQR\": IQR_A\n",
    "   }\n",
    "\n",
    "    stats_A_all = pd.DataFrame(data_A_all, index=M)\n",
    "\n",
    "    data_B_all = {\n",
    "    \"Empirical Mean\": empirical_means_B,\n",
    "    \"Standard Deviation\": standard_deviations_B,\n",
    "    \"Median\": medians_B,\n",
    "    \"Q1\": Q1_B,\n",
    "    \"Q3\": Q3_B,\n",
    "    \"IQR\": IQR_B\n",
    "   }\n",
    "\n",
    "    stats_B_all = pd.DataFrame(data_B_all, index=M)\n",
    "\n",
    "    df_empirical_means_A = pd.DataFrame(empirical_means_A, index=M, columns=beta2_values).transpose()\n",
    "    df_empirical_standard_deviations_A = pd.DataFrame(standard_deviations_A, index=M, columns=beta2_values).transpose()\n",
    "    df_medians_A = pd.DataFrame(medians_A, index=M, columns=beta2_values).transpose()\n",
    "    df_Q1_A = pd.DataFrame(Q1_A, index=M, columns=beta2_values).transpose()\n",
    "    df_Q3_A = pd.DataFrame(Q3_A, index=M, columns=beta2_values).transpose()\n",
    "    df_IQR_A = pd.DataFrame(IQR_A, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "    df_empirical_means_B = pd.DataFrame(empirical_means_B, index=M, columns=beta2_values).transpose()\n",
    "    df_empirical_standard_deviations_B = pd.DataFrame(standard_deviations_B, index=M, columns=beta2_values).transpose()\n",
    "    df_medians_B = pd.DataFrame(medians_B, index=M, columns=beta2_values).transpose()\n",
    "    df_Q1_B = pd.DataFrame(Q1_B, index=M, columns=beta2_values).transpose()\n",
    "    df_Q3_B = pd.DataFrame(Q3_B, index=M, columns=beta2_values).transpose()\n",
    "    df_IQR_B = pd.DataFrame(IQR_B, index=M, columns=beta2_values).transpose()    \n",
    "\n",
    "    return stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \\\n",
    "    stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B\n",
    "\n",
    "\n",
    "(stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \n",
    "    stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B) = create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First shot looking at our Standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Premier subplot : Comparaison des valeurs de deviations standard selon beta2\n",
    "axs[0, 0].plot(beta2_values, df_empirical_standard_deviations_A[2000], label=\"std error A\", color = \"black\")\n",
    "axs[0, 0].plot(beta2_values, df_empirical_standard_deviations_B[2000], label=\"std error B\", color = \"yellow\")\n",
    "axs[0, 0].set_title(\"Comparaison des valeurs de deviations standard selon beta2\")\n",
    "axs[0, 0].grid()\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Deuxième subplot : Quartiles de l'erreur selon le modèle\n",
    "axs[0, 1].plot(beta2_values, df_Q1_A[2000], label=\"Q1 error A\")\n",
    "axs[0, 1].plot(beta2_values, df_Q1_B[2000], label=\"Q1 error B\")\n",
    "axs[0, 1].plot(beta2_values, df_medians_A[2000], label=\"median error A\")\n",
    "axs[0, 1].plot(beta2_values, df_medians_B[2000], label=\"median error B\")\n",
    "axs[0, 1].plot(beta2_values, df_Q3_A[2000], label=\"Q3 error A\")\n",
    "axs[0, 1].plot(beta2_values, df_Q3_B[2000], label=\"Q3 error B\")\n",
    "axs[0, 1].set_title(\"Quartiles de l'erreur selon le modèle\")\n",
    "axs[0, 1].grid()\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Troisième subplot : Quartiles Q1 de l'erreur selon le modèle\n",
    "axs[1, 0].plot(beta2_values, df_Q1_A[2000], label=\"Q1 error A\")\n",
    "axs[1, 0].plot(beta2_values, df_Q1_B[2000], label=\"Q1 error B\")\n",
    "axs[1, 0].set_title(\"Quartiles Q1 de l'erreur selon le modèle\")\n",
    "axs[1, 0].grid()\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Quatrième subplot : Quartiles Q3 de l'erreur selon le modèle\n",
    "axs[1, 1].plot(beta2_values, df_Q3_A[2000], label=\"Q3 error A\", color = \"purple\")\n",
    "axs[1, 1].plot(beta2_values, df_Q3_B[2000], label=\"Q3 error B\", color = \"brown\")\n",
    "axs[1, 1].set_title(\"Quartiles Q3 de l'erreur selon le modèle\")\n",
    "axs[1, 1].grid()\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferential statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the standard error of the mean (according to m and beta2), that gives us a first information of the precisation of mA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive formulas (or algorithms) for the standard error (SE) of the mean\n",
    "\n",
    "Methods: \n",
    "- parametric estimates (consider different cases when n is small versus large and when VA is assumed to be Gaussian or not)\n",
    "- bootstrap estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Parametric estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when n_test is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_by_sqrt(df):\n",
    "    return df.apply(lambda x: x / np.sqrt(x.name), axis=0)\n",
    "\n",
    "def standard_error_m_large():\n",
    "\n",
    "    (stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \n",
    "    stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B) = create_data()\n",
    "\n",
    "    df_standard_error_A = divide_by_sqrt(df_empirical_standard_deviations_A)\n",
    "    df_standard_error_B = divide_by_sqrt(df_empirical_standard_deviations_B)\n",
    "\n",
    "    return df_standard_error_A, df_standard_error_B\n",
    "\n",
    "\n",
    "df_standard_error_A, df_standard_error_B = standard_error_m_large()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when n_test is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_test is small and residual is not gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 / Non parametric estimates - Bootstrap estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error_bootstrap_test():\n",
    "   \n",
    "   n = 150\n",
    "   np.random.seed(42)\n",
    "\n",
    "   M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "   B = 250\n",
    "\n",
    "   se_bootstrap_A = []\n",
    "   se_bootstrap_B = []\n",
    "\n",
    "   for m in M:\n",
    "   \n",
    "      liste_A = []\n",
    "      liste_B = []\n",
    "      beta1 = 2\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      epsilon_train = epsilon[:n]\n",
    "      x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "      bootstrap_indices = np.random.choice(m, size=(B, m), replace=True)\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "                \n",
    "            liste_boot_A = []\n",
    "            liste_boot_B = []\n",
    "            y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          \n",
    "            for bootstrap_index in bootstrap_indices:\n",
    "         \n",
    "               x_test = x[bootstrap_index]\n",
    "               y_test = beta1 * x_test + beta2 * x_test**2 + epsilon[bootstrap_index]\n",
    "            \n",
    "               model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "               y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "               pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "               \n",
    "               x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "               model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "               y_pred_b = model_b.predict(x_test_quad)\n",
    "               pb = (y_test - y_pred_b)**2\n",
    "\n",
    "               sea = np.std(pa, ddof=1) / (m**0.5)\n",
    "               seb = np.std(pb, ddof = 1) / (m**0.5)\n",
    "\n",
    "               liste_boot_A.append(sea)\n",
    "               liste_boot_B.append(seb)\n",
    "\n",
    "            liste_A.append(np.mean(liste_boot_A))\n",
    "            liste_B.append(np.mean(liste_boot_B))\n",
    "    \n",
    "      se_bootstrap_A.append(liste_A)\n",
    "      se_bootstrap_B.append(liste_B)\n",
    "\n",
    "   df_se_bootstrap_A = pd.DataFrame(se_bootstrap_A, index=M, columns=beta2_values).transpose()\n",
    "   df_se_bootstrap_B = pd.DataFrame(se_bootstrap_B, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_se_bootstrap_A, df_se_bootstrap_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap on train sample (pas utile pour l'instant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_each_bootstrap(n, B, m):\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    se_bootstrap_A = []\n",
    "    se_bootstrap_B = []\n",
    "\n",
    "    beta1 = 2\n",
    "    B = 250\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    bootstrap_indices = np.random.choice(n, size=(B, n), replace=True) # bootstrapping\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        liste_A = []\n",
    "        liste_B = []\n",
    "\n",
    "        for bootstrap_index in bootstrap_indices:\n",
    "       \n",
    "\n",
    "           x_train = x[bootstrap_index]\n",
    "           y_train = beta1 * x_train + beta2 * x_train**2 + epsilon[bootstrap_index]\n",
    "      \n",
    "           y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "           y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "           y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "           pa = (y_test - y_pred_a)**2\n",
    "\n",
    "           x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "           x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "           model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "           y_pred_b = model_b.predict(x_test_quad)\n",
    "           pb = (y_test - y_pred_b)**2\n",
    "\n",
    "           sea = np.std(pa, ddof=1) / (n**0.5)\n",
    "           seb = np.std(pb, ddof = 1) / (n**0.5)\n",
    "\n",
    "           liste_A.append(sea)\n",
    "           liste_B.append(seb)\n",
    "\n",
    "        se_bootstrap_A.append(sum(liste_A)/B)\n",
    "        se_bootstrap_B.append(sum(liste_B)/B)\n",
    "    \n",
    "    return se_bootstrap_A, se_bootstrap_B     \n",
    "\n",
    "\n",
    "def standard_error_bootstrap_train():\n",
    "\n",
    "   M = [50, 100, 150, 200, 500, 1000, 2000]\n",
    "\n",
    "   liste_se_bootstrap_A = []\n",
    "   liste_se_bootstrap_B = []\n",
    "\n",
    "   for m in M:\n",
    "\n",
    "      se_bootstrap_A, se_bootstrap_B = se_each_bootstrap(150,250,m)\n",
    "\n",
    "      liste_se_bootstrap_A.append(se_bootstrap_A)\n",
    "      liste_se_bootstrap_B.append(se_bootstrap_B)\n",
    "\n",
    "   df_se_bootstrap_A_train = pd.DataFrame(liste_se_bootstrap_A, index=M, columns=beta2_values).transpose()\n",
    "   df_se_bootstrap_B_train = pd.DataFrame(liste_se_bootstrap_B, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_se_bootstrap_A_train, df_se_bootstrap_B_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se_bootstrap_A_test, df_se_bootstrap_B_test = standard_error_bootstrap_test() \n",
    "df_standard_error_A, df_standard_error_B = standard_error_m_large()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Première figure : Comparaison des valeurs d'erreur standard obtenues selon la méthode\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax1.plot(beta2_values, df_standard_error_A[2000], label=\"Méthode paramétrique\", color=\"blue\")\n",
    "ax1.plot(beta2_values, df_se_bootstrap_A_test[2000], label=\"Méthode bootstrap sur ensemble de test\", color=\"red\")\n",
    "ax1.set_title(\"Erreur standard pour A selon la méthode\")\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "# Sauvegarde ou affichage de la première figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Deuxième figure : Comparaison des valeurs d'erreur standard selon le bootstrapping\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
    "ax2.plot(beta2_values, df_standard_error_A[2000], label=\"Méthode paramétrique A\", color=\"blue\")\n",
    "ax2.plot(beta2_values, df_se_bootstrap_A_test[2000], label=\"Méthode bootstrap A sur ensemble de test\", color=\"red\")\n",
    "ax2.plot(beta2_values, df_standard_error_B[2000], label=\"Méthode paramétrique B\", color=\"darkblue\")\n",
    "ax2.plot(beta2_values, df_se_bootstrap_B_test[2000], label=\"Méthode bootstrap B sur ensemble de test\", color=\"darkred\")\n",
    "ax2.set_title(\"Erreur standard pour A selon le bootstrapping\")\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "# Sauvegarde ou affichage de la deuxième figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2_1 = 0.0\n",
    "beta2_2 = 0.08\n",
    "beta2_3 = 0.16\n",
    "\n",
    "df1 = df_standard_error_A.loc[beta2_1]\n",
    "df2 = df_standard_error_A.loc[beta2_2]\n",
    "df3 = df_standard_error_A.loc[beta2_3]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df1.index, df1.values, marker='o', label = \"evolution de l'erreur standard selon m pour beta2 = \"  f'{beta2_1}')\n",
    "plt.plot(df2.index, df2.values, marker='o', label = \"evolution de l'erreur standard selon m pour beta2 = \" f'{beta2_2}')\n",
    "plt.plot(df3.index, df3.values, marker='o', label = \"evolution de l'erreur standard selon m pour beta2 = \" f'{beta2_3}')\n",
    "plt.title(f'Evolution de l\\'erreur standard du modèle quadratique pour beta2 = {beta2_1}, {beta2_2}, {beta2_3}')\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('Erreur standard')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets of training sets (varying k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error_varying_test_derived():\n",
    "\n",
    "    liste_subsets = [150,200,500,1000,2000]\n",
    "\n",
    "    n = 150\n",
    "    m = 2000\n",
    "    standard_errors_A = []\n",
    "    standard_errors_B = []\n",
    "    np.random.seed(42)\n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    for k in liste_subsets:\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+k]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+k]\n",
    "\n",
    "      liste_A = []\n",
    "      liste_B = []\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "        # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "        # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          se_a = np.std(pa, ddof=1)/(k**0.5)\n",
    "          se_b = np.std(pb, ddof = 1)/(k**0.5)\n",
    "\n",
    "          liste_A.append(se_a)\n",
    "          liste_B.append(se_b)\n",
    "\n",
    "      standard_errors_A.append(liste_A)\n",
    "      standard_errors_B.append(liste_B)\n",
    "\n",
    "    df_se_varying_k_A = pd.DataFrame(standard_errors_A, index=liste_subsets, columns=beta2_values).transpose()\n",
    "    df_se_varying_k_B = pd.DataFrame(standard_errors_B, index=liste_subsets, columns=beta2_values).transpose()\n",
    "\n",
    "    return df_se_varying_k_A, df_se_varying_k_B\n",
    "\n",
    "\n",
    "df_se_varying_k_A, df_se_varying_k_B = standard_error_varying_test_derived()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subsets = [150,200,500,1000,2000]\n",
    "beta2_1 = 0.0\n",
    "beta2_2 = 0.08\n",
    "beta2_3 = 0.16\n",
    "\n",
    "df1 = df_se_varying_k_A.loc[beta2_1]\n",
    "df2 = df_se_varying_k_A.loc[beta2_2]\n",
    "df3 = df_se_varying_k_A.loc[beta2_3]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df1.index, df1.values, marker='o', label = \"evolution de l'erreur standard selon k pour beta2 = \"  f'{beta2_1}')\n",
    "plt.plot(df2.index, df2.values, marker='o', label = \"evolution de l'erreur standard selon k pour beta2 = \" f'{beta2_2}')\n",
    "plt.plot(df3.index, df3.values, marker='o', label = \"evolution de l'erreur standard selon k pour beta2 = \" f'{beta2_3}')\n",
    "plt.title(f'Evolution de l\\'erreur standard du modèle linéaire pour beta2 = {beta2_1}, {beta2_2}, {beta2_3}')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Erreur standard')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying k + Bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapped on train ensemble (pas utile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_each_bootstrap_varying(k):\n",
    "\n",
    "    n = 150\n",
    "    np.random.seed(42)\n",
    "    m = 2000\n",
    "\n",
    "    se_bootstrap_A = []\n",
    "    se_bootstrap_B = []\n",
    "\n",
    "    beta1 = 2\n",
    "    B = 250\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+k]\n",
    "\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+k]\n",
    "\n",
    "    bootstrap_indices = np.random.choice(n, size=(B, n), replace=True) # bootstrapping\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        liste_A = []\n",
    "        liste_B = []\n",
    "\n",
    "        for bootstrap_index in bootstrap_indices:\n",
    "       \n",
    "\n",
    "           x_train = x[bootstrap_index]\n",
    "           y_train = beta1 * x_train + beta2 * x_train**2 + epsilon[bootstrap_index]\n",
    "      \n",
    "           y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "           y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "           y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "           pa = (y_test - y_pred_a)**2\n",
    "\n",
    "           x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "           x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "           model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "           y_pred_b = model_b.predict(x_test_quad)\n",
    "           pb = (y_test - y_pred_b)**2\n",
    "\n",
    "           sea = np.std(pa, ddof=1) / (n**0.5)\n",
    "           seb = np.std(pb, ddof = 1) / (n**0.5)\n",
    "\n",
    "           liste_A.append(sea)\n",
    "           liste_B.append(seb)\n",
    "\n",
    "        se_bootstrap_A.append(sum(liste_A)/B)\n",
    "        se_bootstrap_B.append(sum(liste_B)/B)\n",
    "    \n",
    "    return se_bootstrap_A, se_bootstrap_B     \n",
    "\n",
    "\n",
    "def standard_error_varying_test_bootstrapped_train():\n",
    "\n",
    "   K = [50,100,150,200,500,1000,2000]\n",
    "\n",
    "   liste_se_bootstrap_A = []\n",
    "   liste_se_bootstrap_B = []\n",
    "\n",
    "   for k in K:\n",
    "\n",
    "      se_bootstrap_A, se_bootstrap_B = se_each_bootstrap_varying(k)\n",
    "\n",
    "      liste_se_bootstrap_A.append(se_bootstrap_A)\n",
    "      liste_se_bootstrap_B.append(se_bootstrap_B)\n",
    "\n",
    "   df_se_varying_bootstrap_A_train = pd.DataFrame(liste_se_bootstrap_A, index=K, columns=beta2_values).transpose()\n",
    "   df_se_varying_bootstrap_B_train = pd.DataFrame(liste_se_bootstrap_B, index=K, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_se_varying_bootstrap_A_train, df_se_varying_bootstrap_B_train\n",
    "\n",
    "df_se_varying_bootstrap_A_train, df_se_varying_bootstrap_B_train = standard_error_varying_test_bootstrapped_train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapped on test ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standard_error_varying_bootstrapped_test():\n",
    "   \n",
    "   n = 150\n",
    "   np.random.seed(42)\n",
    "   m = 2000\n",
    "\n",
    "   B = 250\n",
    "\n",
    "   K = [150,200,500,1000,2000]\n",
    "\n",
    "   se_bootstrap_A = []\n",
    "   se_bootstrap_B = []\n",
    "\n",
    "   for k in K:\n",
    "   \n",
    "      liste_A = []\n",
    "      liste_B = []\n",
    "      beta1 = 2\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+k]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+k]\n",
    "\n",
    "      x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "\n",
    "      bootstrap_indices = np.random.choice(k, size=(B, k), replace=True)\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "                \n",
    "            liste_boot_A = []\n",
    "            liste_boot_B = []\n",
    "            y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          \n",
    "            for bootstrap_index in bootstrap_indices:\n",
    "         \n",
    "               \n",
    "               x_test = x[bootstrap_index]\n",
    "               y_test = beta1 * x_test + beta2 * x_test**2 + epsilon[bootstrap_index]\n",
    "            \n",
    "               model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "               y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "               pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "               \n",
    "               x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "               model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "               y_pred_b = model_b.predict(x_test_quad)\n",
    "               pb = (y_test - y_pred_b)**2\n",
    "\n",
    "               sea = np.std(pa, ddof=1) / (k**0.5)\n",
    "               seb = np.std(pb, ddof = 1) / (k**0.5)\n",
    "\n",
    "               liste_boot_A.append(sea)\n",
    "               liste_boot_B.append(seb)\n",
    "\n",
    "            liste_A.append(np.mean(liste_boot_A))\n",
    "            liste_B.append(np.mean(liste_boot_B))\n",
    "    \n",
    "      se_bootstrap_A.append(liste_A)\n",
    "      se_bootstrap_B.append(liste_B)\n",
    "\n",
    "   df_se_bootstrap_A = pd.DataFrame(se_bootstrap_A, index=K, columns=beta2_values).transpose()\n",
    "   df_se_bootstrap_B = pd.DataFrame(se_bootstrap_B, index=K, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_se_bootstrap_A, df_se_bootstrap_B\n",
    "\n",
    "df_se_varying_bootstrap_A_test, df_se_varying_bootstrap_B_test = standard_error_varying_bootstrapped_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between bootstrapped method and derived method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_se_varying_bootstrap_A_test.loc[beta2_1]\n",
    "df2 = df_se_varying_bootstrap_A_test.loc[beta2_2]\n",
    "df3 = df_se_varying_bootstrap_A_test.loc[beta2_3]\n",
    "df4 = df_se_varying_k_A.loc[beta2_1]\n",
    "df5 = df_se_varying_k_A.loc[beta2_2]\n",
    "df6 = df_se_varying_k_A.loc[beta2_3]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df1.index, df1.values, marker='o', label = \"Bootstrap pour beta2 = \"  f'{beta2_1}', color = \"darkblue\")\n",
    "plt.plot(df1.index, df4.values, marker='o', label = \"Derived pour beta2 = \"  f'{beta2_1}', color = \"lightblue\")\n",
    "plt.plot(df2.index, df2.values, marker='o', label = \"Bootstrap pour beta2 = \" f'{beta2_2}', color = \"darkred\")\n",
    "plt.plot(df2.index, df5.values, marker='o', label = \"Derived pour beta2 = \" f'{beta2_2}', color = \"#FF6666\")\n",
    "plt.plot(df3.index, df3.values, marker='o', label = \"Bootstrap pour beta2 = \" f'{beta2_3}', color = \"darkgreen\")\n",
    "plt.plot(df3.index, df6.values, marker='o', label = \"Derived pour beta2 = \" f'{beta2_3}', color = \"lightgreen\")\n",
    "plt.title('Evolution de l\\'erreur standard du modèle varying k bootstrapped')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Erreur standard')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between varying test samples and varying test size (corresponds when k = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Comparison of varying method vs reducing test size\")\n",
    "plt.plot(beta2_values, df_se_bootstrap_A_test[200], label = \"m = 250, smaller size test sample\")\n",
    "plt.plot(beta2_values, df_se_varying_bootstrap_A_test[200], label = \"m = 2000, k = 250, k < m method\")\n",
    "plt.plot()\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison for a given value \n",
    "df4 = df_se_varying_k_A.loc[beta2_1]\n",
    "df5 = df_se_varying_k_A.loc[beta2_2]\n",
    "df6 = df_se_varying_k_A.loc[beta2_3]\n",
    "dff1 = df_standard_error_A.loc[beta2_1]\n",
    "dff2 = df_standard_error_A.loc[beta2_2]\n",
    "dff3 = df_standard_error_A.loc[beta2_3]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Comparison of standard errors derived method\")\n",
    "\n",
    "\n",
    "plt.plot(df4.index, df1.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_1}', color = \"darkred\") # confondue\n",
    "plt.plot(dff1.index, dff1.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_1}', color = \"red\")\n",
    "plt.plot(df5.index, df2.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_2}', color = \"darkblue\")\n",
    "plt.plot(dff2.index, dff2.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_2}', color = \"blue\")\n",
    "plt.plot(df6.index, df3.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_3}', color = \"darkgreen\")\n",
    "plt.plot(dff3.index, dff3.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_3}', color = \"lightgreen\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison for a given value \n",
    "df1 = df_se_varying_bootstrap_A_test.loc[beta2_1]\n",
    "dff1 = df_se_bootstrap_A_test.loc[beta2_1]\n",
    "df2 = df_se_varying_bootstrap_A_test.loc[beta2_2]\n",
    "dff2 = df_se_bootstrap_A_test.loc[beta2_2]\n",
    "df3 = df_se_varying_bootstrap_A_test.loc[beta2_3]\n",
    "dff3 = df_se_bootstrap_A_test.loc[beta2_3]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Comparison of standard errors bootstrap test\")\n",
    "\n",
    "\n",
    "plt.plot(df1.index, df1.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_1}', color = \"darkred\") # confondue\n",
    "plt.plot(dff1.index, dff1.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_1}', color = \"red\")\n",
    "plt.plot(df2.index, df2.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_2}', color = \"darkblue\")\n",
    "plt.plot(dff2.index, dff2.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_2}', color = \"blue\")\n",
    "plt.plot(df3.index, df3.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_3}', color = \"darkgreen\")\n",
    "plt.plot(dff3.index, dff3.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_3}', color = \"lightgreen\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval of the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interval(lower, upper):\n",
    "      return pd.Interval(left=lower, right=upper, closed='both')\n",
    "\n",
    "\n",
    "def confidence_interval_mean():\n",
    "     \n",
    "     \n",
    "   (stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \n",
    "     stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B) = create_data()\n",
    "    \n",
    "   df_standard_error_A, df_standard_error_B = standard_error_m_large()\n",
    "\n",
    "   t_critical = 1.96\n",
    "   CI_lower_A = df_empirical_means_A - t_critical * df_standard_error_A\n",
    "   CI_lower_B = df_empirical_means_B - t_critical * df_standard_error_B\n",
    "   CI_upper_A = df_empirical_means_A + t_critical * df_standard_error_A\n",
    "   CI_upper_B = df_empirical_means_B + t_critical * df_standard_error_B\n",
    "     \n",
    "   confidence_intervals_A = pd.DataFrame(index=CI_lower_A.index, columns=CI_lower_A.columns)\n",
    "   confidence_intervals_B = pd.DataFrame(index=CI_lower_B.index, columns=CI_lower_B.columns)\n",
    "\n",
    "   for col in CI_lower_A.columns:\n",
    "    confidence_intervals_A[col] = CI_lower_A[col].combine(CI_upper_A[col], create_interval)\n",
    "\n",
    "   for col in CI_lower_B.columns:\n",
    "    confidence_intervals_B[col] = CI_lower_B[col].combine(CI_upper_B[col], create_interval)\n",
    "\n",
    "   return confidence_intervals_A, confidence_intervals_B\n",
    "\n",
    "confidence_intervals_A, confidence_intervals_B = confidence_interval_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_each_bootstrap(n, B, m):\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ci_bootstrap_A = []\n",
    "    ci_bootstrap_B = []\n",
    "\n",
    "    beta1 = 2\n",
    "    B = 250\n",
    "    alpha = 0.05\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    bootstrap_indices = np.random.choice(m, size=(B, m), replace=True) # bootstrapping\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        moy_boot_A = []\n",
    "        moy_boot_B = []\n",
    "        y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "\n",
    "        for bootstrap_index in bootstrap_indices:\n",
    "       \n",
    "           x_test = x[bootstrap_index]\n",
    "           y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "           y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "           pa = (y_test - y_pred_a)**2\n",
    "\n",
    "           x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "           x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "           model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "           y_pred_b = model_b.predict(x_test_quad)\n",
    "           pb = (y_test - y_pred_b)**2\n",
    "    \n",
    "           moy_boot_A.append(np.mean(pa))\n",
    "           moy_boot_B.append(np.mean(pb))\n",
    "\n",
    "        lower_A = np.percentile(moy_boot_A, 100 * (alpha / 2))\n",
    "        upper_A = np.percentile(moy_boot_A, 100 * (1 - alpha / 2))\n",
    "        lower_B = np.percentile(moy_boot_B, 100 * (alpha / 2))\n",
    "        upper_B = np.percentile(moy_boot_B, 100 * (1 - alpha / 2))\n",
    "\n",
    "        ci_bootstrap_A.append(pd.Interval(left=lower_A, right=upper_A, closed='both'))\n",
    "        ci_bootstrap_B.append(pd.Interval(left=lower_B, right=upper_B, closed='both'))\n",
    "    \n",
    "    return ci_bootstrap_A, ci_bootstrap_B     \n",
    "\n",
    "def confidence_interval_bootstrap():\n",
    "\n",
    "   M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "   liste_ci_bootstrap_A = []\n",
    "   liste_ci_bootstrap_B = []\n",
    "\n",
    "   for m in M:\n",
    "\n",
    "      ci_bootstrap_A, ci_bootstrap_B = conf_each_bootstrap(150,250,m)\n",
    "\n",
    "      liste_ci_bootstrap_A.append(ci_bootstrap_A)\n",
    "      liste_ci_bootstrap_B.append(ci_bootstrap_B)\n",
    "\n",
    "   df_ci_bootstrap_A = pd.DataFrame(liste_ci_bootstrap_A, index=M, columns=beta2_values).transpose()\n",
    "   df_ci_bootstrap_B = pd.DataFrame(liste_ci_bootstrap_B, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_ci_bootstrap_A, df_ci_bootstrap_B\n",
    "\n",
    "df_ci_bootstrap_A, df_ci_bootstrap_B = confidence_interval_bootstrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential statistics : is a model better than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval for the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different subsets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_simulation():\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n= 150\n",
    "    m = 2000\n",
    "    power = []\n",
    "    standard_errors = []\n",
    "    test_statistics = []\n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        \n",
    "        y_train = beta1 * x_train + beta2 * x_train **2 + epsilon_train\n",
    "        y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "        # Linear Model\n",
    "        model_a1 = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "        y_pred_a1 = model_a1.predict(x_test.reshape(-1, 1))\n",
    "        pa = (y_test-y_pred_a1)**2\n",
    "\n",
    "            # Quadratic Model\n",
    "        x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "        x_test_quad = np.column_stack((x_test, x_test**2))    \n",
    "        model_a2 = LinearRegression().fit(x_train_quad, y_train)\n",
    "        y_pred_a2 = model_a2.predict(x_test_quad)\n",
    "        pb = (y_test-y_pred_a2)**2\n",
    "\n",
    "        diff = pa - pb\n",
    "\n",
    "        d_barre = np.mean(diff)\n",
    "        se = np.std(diff)/(m**0.5)\n",
    " \n",
    "        test_stat = d_barre/se # Statistique pour l'hypothèse nulle\n",
    "\n",
    "        student = stats.t.ppf(0.95, df=m-1)\n",
    "        test_alt = student - test_stat\n",
    "        puissance = 1 - stats.norm.cdf(test_alt, 0, 1)\n",
    "       \n",
    "        standard_errors.append(se)\n",
    "        test_statistics.append(test_stat)\n",
    "        power.append(puissance)\n",
    "\n",
    "    return  standard_errors, test_statistics, power\n",
    "\n",
    "\n",
    "standard_errors, test_statistics, power = power_simulation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Puissance du test selon beta2\")\n",
    "plt.plot(beta2_values, power, label = \"puissance\", color = \"red\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_simulation_boot():\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n = 150\n",
    "    m = 2000\n",
    "    power = []\n",
    "    standard_errors = []\n",
    "    test_statistics = []\n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    B = 250\n",
    "    bootstrap_indices = np.random.choice(m, size=(B, m), replace=True) # bootstrapping\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        moy_boot_diff = []\n",
    "        y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "\n",
    "        for bootstrap_index in bootstrap_indices:\n",
    "       \n",
    "           x_test = x[bootstrap_index]\n",
    "           y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "           y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "           pa = (y_test - y_pred_a)**2\n",
    "\n",
    "           x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "           x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "           model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "           y_pred_b = model_b.predict(x_test_quad)\n",
    "           pb = (y_test - y_pred_b)**2\n",
    "\n",
    "           diff = pa - pb\n",
    "    \n",
    "           moy_boot_diff.append(np.mean(diff))\n",
    "\n",
    "        d_barre = np.mean(moy_boot_diff)\n",
    "        se = np.std(moy_boot_diff)/(m**0.5)\n",
    " \n",
    "        test_stat = d_barre/se # Statistique pour l'hypothèse nulle\n",
    "\n",
    "        student = stats.t.ppf(0.95, df=m-1)\n",
    "        test_alt = student - test_stat\n",
    "        puissance = 1 - stats.norm.cdf(test_alt, 0, 1)\n",
    "       \n",
    "        standard_errors.append(se)\n",
    "        test_statistics.append(test_stat)\n",
    "        power.append(puissance)\n",
    "\n",
    "    return  standard_errors, test_statistics, power\n",
    "\n",
    "\n",
    "standard_errors_boot, test_statistics_boot, power_boot = power_simulation_boot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Puissance du test selon beta2\")\n",
    "plt.plot(beta2_values, power_boot, label = \"puissance\", color = \"darkred\")\n",
    "plt.plot(beta2_values, power, label = \"puissance\", color = \"red\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

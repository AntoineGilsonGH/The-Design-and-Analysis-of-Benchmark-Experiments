{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Bibliothèques nécéssaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import uniform, norm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Introduction, variables, training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des premières variables\n",
    "\n",
    "beta1 = 2\n",
    "beta2_values = np.linspace(0, 0.16, 9)\n",
    "M = [150,200,500,1000,2000]\n",
    "n = 150\n",
    "m = 2000\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des ensembles d'entrainement, de test, coefficients des modèles \n",
    "\n",
    "np.random.seed(42)\n",
    "epsilon = np.random.normal(0, 1, n + m) \n",
    "x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "x_train = x[:n]\n",
    "x_test = x[n:n+m]\n",
    "\n",
    "epsilon_train = epsilon[:n]\n",
    "epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "modele_A = []\n",
    "modele_B = []\n",
    "\n",
    "\n",
    "for beta2 in beta2_values:\n",
    "            \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "\n",
    "          coef_a = model_a.coef_[0]\n",
    "          intercept_a = model_a.intercept_     \n",
    "        \n",
    "          # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          coef_b = model_b.coef_\n",
    "          intercept_b = model_b.intercept_\n",
    "\n",
    "          y = beta1*x + beta2*x**2 +epsilon \n",
    "          eq_a = intercept_a + coef_a * x\n",
    "          modele_A.append([intercept_a, coef_a])\n",
    "          eq_a_train = intercept_a + coef_a * x_train\n",
    "          eq_a_test = intercept_a + coef_a * x_test\n",
    "          eq_b = intercept_b + coef_b[0] * x + coef_b[1] * x**2\n",
    "          modele_B.append([intercept_b, coef_b])\n",
    "          eq_b_train = intercept_b + coef_b[0] * x_train + coef_b[1] * x_train**2\n",
    "          eq_b_test = intercept_b + coef_b[0] * x_test + coef_b[1] * x_test**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que la distribution de nos variables aléatoires est bien cohérente\n",
    "\n",
    "def print_random_variables():\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    \n",
    "    epsilon = np.random.normal(0, 1, n + m)\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    x_vals = np.linspace(0, 5, 100)\n",
    "    epsilon_vals = np.linspace(-4, 4, 100)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Distribution de X\")\n",
    "    plt.hist(x_train, bins=30, label=\"X Training set n_train = 150\", color=\"blue\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.hist(x_test, bins=30, label=\"X Test set n_test = 2000\", color=\"red\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.plot(x_vals, uniform.pdf(x_vals, 0, 5), 'k-', label=\"Uniform(0, 5)\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Distribution de epsilon\")\n",
    "    plt.hist(epsilon_train, bins=30, label=\"Eps Training set n_train = 150\", color=\"green\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.hist(epsilon_test, bins=30, label=\"Eps Test set m_train = 2000\", color=\"orange\", edgecolor='black', alpha=0.7, density=True)\n",
    "    plt.plot(epsilon_vals, norm.pdf(epsilon_vals, 0, 1), 'k-', label=\"Normal(0, 1)\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les fits de nos modèles \n",
    "\n",
    "def print_training_test_sets(): \n",
    "    \n",
    "\n",
    "      np.random.seed(42)\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+m]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+m]\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "\n",
    "          coef_a = model_a.coef_[0]\n",
    "          intercept_a = model_a.intercept_     \n",
    "        \n",
    "          # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          coef_b = model_b.coef_\n",
    "          intercept_b = model_b.intercept_\n",
    "\n",
    "          y = beta1*x + beta2*x**2 +epsilon \n",
    "          eq_a = intercept_a + coef_a * x\n",
    "          eq_a_train = intercept_a + coef_a * x_train\n",
    "          eq_a_test = intercept_a + coef_a * x_test\n",
    "          eq_b = intercept_b + coef_b[0] * x + coef_b[1] * x**2\n",
    "          eq_b_train = intercept_b + coef_b[0] * x_train + coef_b[1] * x_train**2\n",
    "          eq_b_test = intercept_b + coef_b[0] * x_test + coef_b[1] * x_test**2\n",
    "\n",
    "          plt.figure() # Fits des modèles \n",
    "          plt.title(\"Fits d'entrainement des modèles pour \" f\"beta2 = {beta2}\" \" et pour \"f\"n_train= {n}\" )  \n",
    "          plt.scatter(x_train[np.argsort(x_train)], y_train[np.argsort(x_train)], label = \"Y réel\", color = \"green\")\n",
    "          plt.plot(x_train[np.argsort(x_train)], eq_a_train[np.argsort(x_train)], label = \"Prédiction linéaire\", color = \"Red\")\n",
    "          plt.plot(x_train[np.argsort(x_train)], eq_b_train[np.argsort(x_train)], label = \"Prédiction quadratique\", color = \"Orange\")\n",
    "          plt.xlabel(\"X\")\n",
    "          plt.ylabel(\"Y\")\n",
    "          plt.grid()\n",
    "          plt.legend()  \n",
    "          plt.show()     \n",
    "\n",
    "\n",
    "          plt.figure() # Fits des modèles \n",
    "          plt.title(\"Fits de test des modèles pour \" f\"beta2 = {beta2}\" \" et pour \"f\"n_test = {m}\" )  \n",
    "          plt.scatter(x_test[np.argsort(x_test)], y_test[np.argsort(x_test)], label = \"Y réel\", color = \"green\")\n",
    "          plt.plot(x_test[np.argsort(x_test)], eq_a_test[np.argsort(x_test)], label = \"Prédiction linéaire\", color = \"Red\")\n",
    "          plt.plot(x_test[np.argsort(x_test)], eq_b_test[np.argsort(x_test)], label = \"Prédiction quadratique\", color = \"Orange\")\n",
    "          plt.xlabel(\"X\")\n",
    "          plt.ylabel(\"Y\")\n",
    "          plt.grid()\n",
    "          plt.legend()  \n",
    "          plt.show()  \n",
    "\n",
    "          plt.figure() # Fits des modèles \n",
    "          plt.title(\"Fits des modèles pour \" f\"beta2 = {beta2}\" \" et pour \"f\"n_test = {m}\" )  \n",
    "          plt.scatter(x[np.argsort(x)], y[np.argsort(x)], label = \"Y réel\", color = \"green\")\n",
    "          plt.plot(x[np.argsort(x)], eq_a[np.argsort(x)], label = \"Prédiction linéaire\", color = \"Red\")\n",
    "          plt.plot(x[np.argsort(x)], eq_b[np.argsort(x)], label = \"Prédiction quadratique\", color = \"Orange\")\n",
    "          plt.xlabel(\"X\")\n",
    "          plt.ylabel(\"Y\")\n",
    "          plt.grid()\n",
    "          plt.legend()  \n",
    "          plt.show()  \n",
    "\n",
    "print_training_test_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Erreurs de prédiction de chacun des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance des deux modèles conditionnellement à X\n",
    "\n",
    "def performances_metrics_cond_X():\n",
    "\n",
    "    for m in M:\n",
    "\n",
    "        x = np.random.uniform(0, 5, n + m)\n",
    "        x_train = x[:n]\n",
    "\n",
    "        epsilon = np.random.normal(0, 1, n + m)\n",
    "        epsilon_train = epsilon[:n]\n",
    "        x_test = x[n:n + m]\n",
    "        epsilon_test = epsilon[n:n + m]\n",
    "\n",
    "        for beta2 in beta2_values:\n",
    "            \n",
    "\n",
    "            y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "            y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "            # Linear Model : A\n",
    "            model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "            y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "            pa = (y_test - y_pred_a)**2\n",
    "\n",
    "            # Quadratic Model : B\n",
    "            x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "            x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "            model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "            y_pred_b = model_b.predict(x_test_quad)\n",
    "            pb = (y_test - y_pred_b)**2\n",
    "\n",
    "            differences = pa - pb\n",
    "\n",
    "            # Représentation graphique de pa - pb, la différence de performance entre deux modèles \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f\"pa - pb  pour beta2 = {beta2} et pour n_test= {m}\")\n",
    "\n",
    "            sns.scatterplot(x=x_test, y=differences, color=\"red\", s=6, label=f\"Differences pour beta2 = {beta2} et pour n_test = {m}\")\n",
    "            plt.axhline(y=0, color='black', linestyle='--', label='y = 0')\n",
    "\n",
    "            box = plt.boxplot(differences, positions=[max(x_test) + 1], widths=0.5, patch_artist=True,\n",
    "                              boxprops=dict(facecolor='blue', color='blue', alpha=0.5),\n",
    "                              medianprops=dict(color='yellow'),\n",
    "                              whiskerprops=dict(color='blue'),\n",
    "                              capprops=dict(color='blue'),\n",
    "                              flierprops=dict(color='blue', markeredgecolor='blue'))\n",
    "\n",
    "            whisker_min = box['whiskers'][0].get_ydata()[1]\n",
    "            whisker_max = box['whiskers'][1].get_ydata()[1]\n",
    "            q1 = box['boxes'][0].get_path().vertices[0, 1]\n",
    "            median = box['medians'][0].get_ydata()[0]\n",
    "            q3 = box['boxes'][0].get_path().vertices[2, 1]\n",
    "\n",
    "            plt.text(max(x_test) + 1, whisker_min, f'Whisker Min: {whisker_min:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, whisker_max, f'Whisker Max: {whisker_max:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q1, f'Q1: {q1:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, median, f'Median: {median:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q3, f'Q3: {q3:.2f}', horizontalalignment='center', color='black')\n",
    "\n",
    "            plt.xlabel(\"X test set\")\n",
    "            plt.ylabel(\"Differences\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Représentation de pa, l'erreur du modèle linéaire\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f\"pa = (y-ya)**2  pour beta2 = {beta2} et pour n_test= {m}\")\n",
    "\n",
    "            sns.scatterplot(x=x_test, y=pa, color=\"red\", s=6, label=f\"Differences pour beta2 = {beta2} et pour n_test = {m}\")\n",
    "\n",
    "            box = plt.boxplot(pb, positions=[max(x_test) + 1], widths=0.5, patch_artist=True,\n",
    "                              boxprops=dict(facecolor='blue', color='blue', alpha=0.5),\n",
    "                              medianprops=dict(color='yellow'),\n",
    "                              whiskerprops=dict(color='blue'),\n",
    "                              capprops=dict(color='blue'),\n",
    "                              flierprops=dict(color='blue', markeredgecolor='blue'))\n",
    "\n",
    "            whisker_min = box['whiskers'][0].get_ydata()[1]\n",
    "            whisker_max = box['whiskers'][1].get_ydata()[1]\n",
    "            q1 = box['boxes'][0].get_path().vertices[0, 1]\n",
    "            median = box['medians'][0].get_ydata()[0]\n",
    "            q3 = box['boxes'][0].get_path().vertices[2, 1]\n",
    "\n",
    "            plt.text(max(x_test) + 1, whisker_min, f'Whisker Min: {whisker_min:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, whisker_max, f'Whisker Max: {whisker_max:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q1, f'Q1: {q1:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, median, f'Median: {median:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q3, f'Q3: {q3:.2f}', horizontalalignment='center', color='black')\n",
    "\n",
    "            plt.xlabel(\"X test set\")\n",
    "            plt.ylabel(\"pa = (y-ya)**2\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Représentation de pb, l'erreur du modèle quadratique\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f\"pb = (y-yb)**2  pour beta2 = {beta2} et pour n_test= {m}\")\n",
    "\n",
    "            sns.scatterplot(x=x_test, y=pb, color=\"red\", s=6, label=f\"Differences pour beta2 = {beta2} et pour n_test = {m}\")\n",
    "\n",
    "            box = plt.boxplot(pb, positions=[max(x_test) + 1], widths=0.5, patch_artist=True,\n",
    "                              boxprops=dict(facecolor='blue', color='blue', alpha=0.5),\n",
    "                              medianprops=dict(color='yellow'),\n",
    "                              whiskerprops=dict(color='blue'),\n",
    "                              capprops=dict(color='blue'),\n",
    "                              flierprops=dict(color='blue', markeredgecolor='blue'))\n",
    "\n",
    "            whisker_min = box['whiskers'][0].get_ydata()[1]\n",
    "            whisker_max = box['whiskers'][1].get_ydata()[1]\n",
    "            q1 = box['boxes'][0].get_path().vertices[0, 1]\n",
    "            median = box['medians'][0].get_ydata()[0]\n",
    "            q3 = box['boxes'][0].get_path().vertices[2, 1]\n",
    "\n",
    "            plt.text(max(x_test) + 1, whisker_min, f'Whisker Min: {whisker_min:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, whisker_max, f'Whisker Max: {whisker_max:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q1, f'Q1: {q1:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, median, f'Median: {median:.2f}', horizontalalignment='center', color='black')\n",
    "            plt.text(max(x_test) + 1, q3, f'Q3: {q3:.2f}', horizontalalignment='center', color='black')\n",
    "\n",
    "            plt.xlabel(\"X test set\")\n",
    "            plt.ylabel(\"pb = (y-yb)**2\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition de la performance du modèle \n",
    "\n",
    "def performance_metrics_hist():\n",
    "    \n",
    "    for m in M:\n",
    "\n",
    "      np.random.seed(42)\n",
    "      beta1 = 2\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+m]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+m]\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2     \n",
    "        \n",
    "          # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          differences = pa - pb\n",
    "\n",
    "          plt.figure() # Erreur individuelle des modèles\n",
    "          plt.title(\"prédiction du modèle linéaire pour \" f\"beta2 = {beta2}\" \" et pour \"f\"n_test = {m}\" ) \n",
    "          plt.hist(pa, bins=30, label=f\"pa = (y - ya)^2 pour beta2 = {beta2} et pour n_test = {m}\", color=\"red\", edgecolor='black', alpha=0.7)\n",
    "          plt.xlabel(\" Erreur associée \")\n",
    "          plt.ylabel(\"Nombre d'occurences\")      \n",
    "          plt.grid()\n",
    "          plt.legend()\n",
    "          plt.show()\n",
    "\n",
    "          plt.figure()\n",
    "          plt.title(\"prédiction du modèle quadratique pour \" f\"beta2 = {beta2}\" \" et pour \"f\"n_test = {m}\" )       \n",
    "          plt.hist(pb, bins=30, label=f\"pb = (y - yb)^2 pour beta2 = {beta2} et pour n_test = {m}\", color=\"orange\", edgecolor='black', alpha=0.7)\n",
    "          plt.xlabel(\" Erreur associée \")\n",
    "          plt.ylabel(\"Nombre d'occurences\")      \n",
    "          plt.grid()\n",
    "          plt.legend()\n",
    "          plt.show()\n",
    "\n",
    "          plt.figure()\n",
    "          plt.title(\"Différence de prédiction des modèles \" f\"beta2 = {beta2}\" \" et pour \"f\"n_test = {m}\" )        \n",
    "          plt.hist(differences, bins=30, label=f\"pa - pb pour beta2 = {beta2} et pour n_test = {m}\", color=\"blue\", edgecolor='black', alpha=0.7)\n",
    "          plt.xlabel(\" Erreur associée \")\n",
    "          plt.ylabel(\"Nombre d'occurences\")      \n",
    "          plt.grid()\n",
    "          plt.legend()\n",
    "          plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "\n",
    "def MSE():\n",
    "    \n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    mse_A = []\n",
    "    mse_B = []\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "            \n",
    "        y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "        y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "         # Linear Model : A\n",
    "        model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "        y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "\n",
    "        mse_A.append(mean_squared_error(y_pred_a, y_test))\n",
    "    \n",
    "          # Quadratic Model : B\n",
    "        x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "        x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "        model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "        y_pred_b = model_b.predict(x_test_quad)\n",
    "        mse_B.append(mean_squared_error(y_pred_b, y_test))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Différence des MSE\")\n",
    "    plt.plot(beta2_values, mse_A, label = \"MSE_A\")\n",
    "    plt.plot(beta2_values, mse_B, label = \"MSE_B\")\n",
    "       \n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Statistiques descriptives sur l'erreur de prédiction de chacun des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un tableau regroupant différents indicateurs \n",
    "\n",
    "def descriptive_statistics():\n",
    "    \n",
    "    empirical_means_A = []\n",
    "    standard_deviations_A = []\n",
    "    medians_A = []\n",
    "    Q1_A = []\n",
    "    Q3_A = []\n",
    "    IQR_A = []\n",
    "\n",
    "    empirical_means_B = []\n",
    "    standard_deviations_B = []\n",
    "    medians_B = []\n",
    "    Q1_B = []\n",
    "    Q3_B = []\n",
    "    IQR_B = []\n",
    "\n",
    "    for m in M:\n",
    "\n",
    "      a = []\n",
    "      b = []\n",
    "      c = []\n",
    "      d = []\n",
    "      q = []\n",
    "      iqr = []\n",
    "\n",
    "      e = []\n",
    "      f = []\n",
    "      g = []\n",
    "      h = []\n",
    "      q_ = []\n",
    "      iqr_ = []\n",
    "\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+m]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+m]\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "        # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "        # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          mean_a = sum(pa)/m\n",
    "          mean_b = sum(pb)/m\n",
    "\n",
    "          a.append(mean_a)\n",
    "          e.append(mean_b)\n",
    "\n",
    "          std_a = np.std(pa, ddof=1)\n",
    "          std_b = np.std(pb, ddof = 1)\n",
    "\n",
    "          b.append(std_a)\n",
    "          f.append(std_b)\n",
    "\n",
    "          q1_a = np.percentile(pa, 25)\n",
    "          med_a = np.percentile(pa, 50)        \n",
    "          q3_a = np.percentile(pa, 75)\n",
    "\n",
    "          q1_b = np.percentile(pb, 25)\n",
    "          med_b = np.percentile(pb, 50)        \n",
    "          q3_b = np.percentile(pb, 75)\n",
    "\n",
    "          c.append(med_a)\n",
    "          g.append(med_b)\n",
    "          d.append(q1_a)\n",
    "          q.append(q3_a)\n",
    "          h.append(q1_b)\n",
    "          q_.append(q3_b)\n",
    "\n",
    "          iqr.append(q3_a - q1_a)\n",
    "          iqr_.append(q3_b - q1_b)\n",
    "\n",
    "      empirical_means_A.append(a)\n",
    "      empirical_means_B.append(e)\n",
    "      standard_deviations_A.append(b)\n",
    "      standard_deviations_B.append(f)\n",
    "      medians_A.append(c)\n",
    "      medians_B.append(g)\n",
    "      Q1_A.append(d)\n",
    "      Q1_B.append(h)\n",
    "      Q3_A.append(q)\n",
    "      Q3_B.append(q_)\n",
    "      IQR_A.append(iqr)\n",
    "      IQR_B.append(iqr_)\n",
    "\n",
    "    return empirical_means_A, standard_deviations_A, medians_A, Q1_A, Q3_A, IQR_A, empirical_means_B, standard_deviations_B, medians_B, Q1_B, Q3_B, IQR_B\n",
    "\n",
    "def create_data():\n",
    "    \n",
    "    empirical_means_A, standard_deviations_A, medians_A, Q1_A, Q3_A, IQR_A, empirical_means_B, standard_deviations_B, medians_B, Q1_B, Q3_B, IQR_B = descriptive_statistics()\n",
    "    \n",
    "    \n",
    "    data_A_all = {\n",
    "    \"Empirical Mean\": empirical_means_A,\n",
    "    \"Standard Deviation\": standard_deviations_A,\n",
    "    \"Median\": medians_A,\n",
    "    \"Q1\": Q1_A,\n",
    "    \"Q3\": Q3_A,\n",
    "    \"IQR\": IQR_A\n",
    "   }\n",
    "\n",
    "    stats_A_all = pd.DataFrame(data_A_all, index=M)\n",
    "\n",
    "    data_B_all = {\n",
    "    \"Empirical Mean\": empirical_means_B,\n",
    "    \"Standard Deviation\": standard_deviations_B,\n",
    "    \"Median\": medians_B,\n",
    "    \"Q1\": Q1_B,\n",
    "    \"Q3\": Q3_B,\n",
    "    \"IQR\": IQR_B\n",
    "   }\n",
    "\n",
    "    stats_B_all = pd.DataFrame(data_B_all, index=M)\n",
    "\n",
    "    df_empirical_means_A = pd.DataFrame(empirical_means_A, index=M, columns=beta2_values).transpose()\n",
    "    df_empirical_standard_deviations_A = pd.DataFrame(standard_deviations_A, index=M, columns=beta2_values).transpose()\n",
    "    df_medians_A = pd.DataFrame(medians_A, index=M, columns=beta2_values).transpose()\n",
    "    df_Q1_A = pd.DataFrame(Q1_A, index=M, columns=beta2_values).transpose()\n",
    "    df_Q3_A = pd.DataFrame(Q3_A, index=M, columns=beta2_values).transpose()\n",
    "    df_IQR_A = pd.DataFrame(IQR_A, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "    df_empirical_means_B = pd.DataFrame(empirical_means_B, index=M, columns=beta2_values).transpose()\n",
    "    df_empirical_standard_deviations_B = pd.DataFrame(standard_deviations_B, index=M, columns=beta2_values).transpose()\n",
    "    df_medians_B = pd.DataFrame(medians_B, index=M, columns=beta2_values).transpose()\n",
    "    df_Q1_B = pd.DataFrame(Q1_B, index=M, columns=beta2_values).transpose()\n",
    "    df_Q3_B = pd.DataFrame(Q3_B, index=M, columns=beta2_values).transpose()\n",
    "    df_IQR_B = pd.DataFrame(IQR_B, index=M, columns=beta2_values).transpose()    \n",
    "\n",
    "    return stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \\\n",
    "    stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B\n",
    "\n",
    "\n",
    "(stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \n",
    "    stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B) = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de certaines variables selon beta2\n",
    "\n",
    "def plot_variables_beta2():\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    axs[0, 0].plot(beta2_values, df_empirical_standard_deviations_A[2000], label=\"deviation standard A\", color=\"black\")\n",
    "    axs[0, 0].plot(beta2_values, df_empirical_standard_deviations_B[2000], label=\"deviation standard B\", color=\"yellow\")\n",
    "    axs[0, 0].set_title(\"Comparaison des valeurs de déviations standard selon beta2\")\n",
    "    axs[0, 0].grid()\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    axs[0, 1].plot(beta2_values, df_Q1_A[2000], label=\"Q1 error A\")\n",
    "    axs[0, 1].plot(beta2_values, df_Q1_B[2000], label=\"Q1 error B\")\n",
    "    axs[0, 1].plot(beta2_values, df_medians_A[2000], label=\"median error A\")\n",
    "    axs[0, 1].plot(beta2_values, df_medians_B[2000], label=\"median error B\")\n",
    "    axs[0, 1].plot(beta2_values, df_Q3_A[2000], label=\"Q3 error A\")\n",
    "    axs[0, 1].plot(beta2_values, df_Q3_B[2000], label=\"Q3 error B\")\n",
    "    axs[0, 1].set_title(\"Quartiles de l'erreur selon le modèle\")\n",
    "    axs[0, 1].grid()\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 0].plot(beta2_values, df_Q1_A[2000], label=\"Q1 error A\")\n",
    "    axs[1, 0].plot(beta2_values, df_Q1_B[2000], label=\"Q1 error B\")\n",
    "    axs[1, 0].set_title(\"Quartiles Q1 de l'erreur selon le modèle\")\n",
    "    axs[1, 0].grid()\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    axs[1, 1].plot(beta2_values, df_Q3_A[2000], label=\"Q3 error A\", color=\"purple\")\n",
    "    axs[1, 1].plot(beta2_values, df_Q3_B[2000], label=\"Q3 error B\", color=\"brown\")\n",
    "    axs[1, 1].set_title(\"Quartiles Q3 de l'erreur selon le modèle\")\n",
    "    axs[1, 1].grid()\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de certaines variables selon n_test\n",
    "\n",
    "def plot_variables_ntest():\n",
    "    \n",
    "    beta2_1, beta2_2, beta2_3 = [0.0, 0.08, 0.16]\n",
    "    \n",
    "    df1_std = df_empirical_standard_deviations_A.loc[beta2_1]\n",
    "    df2_std = df_empirical_standard_deviations_A.loc[beta2_2]\n",
    "    df3_std = df_empirical_standard_deviations_A.loc[beta2_3]\n",
    "    \n",
    "    df1_med = df_medians_A.loc[beta2_1]\n",
    "    df2_med = df_medians_A.loc[beta2_2]\n",
    "    df3_med = df_medians_A.loc[beta2_3]\n",
    "    \n",
    "    df1_q1 = df_Q1_A.loc[beta2_1]\n",
    "    df2_q1 = df_Q1_A.loc[beta2_2]\n",
    "    df3_q1 = df_Q1_A.loc[beta2_3]\n",
    "    \n",
    "    df1_q3 = df_Q3_A.loc[beta2_1]\n",
    "    df2_q3 = df_Q3_A.loc[beta2_2]\n",
    "    df3_q3 = df_Q3_A.loc[beta2_3]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Premier subplot : Comparaison des valeurs de deviations standard selon beta2\n",
    "    axs[0, 0].plot(M, df1_std, label=\"deviation standard A, beta2 = 0\", color=\"black\")\n",
    "    axs[0, 0].plot(M, df2_std, label=\"deviation standard A beta2 = 0.08\", color=\"yellow\")\n",
    "    axs[0, 0].plot(M, df3_std, label=\"deviation standard A, beta2 = 0.16\", color=\"red\")\n",
    "    axs[0, 0].set_title(\"Comparaison des valeurs de déviations standard selon n_test\")\n",
    "    axs[0, 0].grid()\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Deuxième subplot : Quartiles de l'erreur selon le modèle\n",
    "    axs[0, 1].plot(M, df1_q1, label=\"Q1 error A, beta = 0\")\n",
    "    axs[0, 1].plot(M, df2_q1, label=\"Q1 error A, beta = 0.08\")\n",
    "    axs[0, 1].plot(M, df3_q1, label=\"Q1 error A, beta = 0.16\")\n",
    "    axs[0, 1].plot(M, df1_med, label=\"median error A, beta = 0\")\n",
    "    axs[0, 1].plot(M, df2_med, label=\"median error A, beta = 0.08\")\n",
    "    axs[0, 1].plot(M, df3_med, label=\"median error A, beta = 0.16\")\n",
    "    axs[0, 1].plot(M, df1_q3, label=\"Q3 error A, beta = 0\")\n",
    "    axs[0, 1].plot(M, df2_q3, label=\"Q3 error A, beta = 0.08\")\n",
    "    axs[0, 1].plot(M, df3_q3, label=\"Q3 error A, beta = 0.16\")\n",
    "    axs[0, 1].set_title(\"Quartiles de l'erreur selon le modèle\")\n",
    "    axs[0, 1].grid()\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 0].plot(M, df1_q1, label=\"Q1 error A, beta = 0\")\n",
    "    axs[1, 0].plot(M, df2_q1, label=\"Q1 error A, beta = 0.08\")\n",
    "    axs[1, 0].plot(M, df3_q1, label=\"Q1 error A, beta = 0.16\")\n",
    "    axs[1, 0].set_title(\"Quartiles Q1 de l'erreur selon beta2\")\n",
    "    axs[1, 0].grid()\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    axs[1, 1].plot(M, df1_q3, label=\"Q3 error A, beta = 0\")\n",
    "    axs[1, 1].plot(M, df2_q3, label=\"Q3 error A, beta = 0.08\")\n",
    "    axs[1, 1].plot(M, df3_q3, label=\"Q3 error A, beta = 0.16\")\n",
    "    axs[1, 1].set_title(\"Quartiles Q3 de l'erreur selon le modèle\")\n",
    "    axs[1, 1].grid()\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    df1_std = df_empirical_standard_deviations_B.loc[beta2_1]\n",
    "    df2_std = df_empirical_standard_deviations_B.loc[beta2_2]\n",
    "    df3_std = df_empirical_standard_deviations_B.loc[beta2_3]\n",
    "    \n",
    "    df1_med = df_medians_B.loc[beta2_1]\n",
    "    df2_med = df_medians_B.loc[beta2_2]\n",
    "    df3_med = df_medians_B.loc[beta2_3]\n",
    "    \n",
    "    df1_q1 = df_Q1_B.loc[beta2_1]\n",
    "    df2_q1 = df_Q1_B.loc[beta2_2]\n",
    "    df3_q1 = df_Q1_B.loc[beta2_3]\n",
    "    \n",
    "    df1_q3 = df_Q3_B.loc[beta2_1]\n",
    "    df2_q3 = df_Q3_B.loc[beta2_2]\n",
    "    df3_q3 = df_Q3_B.loc[beta2_3]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Premier subplot : Comparaison des valeurs de deviations standard selon beta2\n",
    "    axs[0, 0].plot(M, df1_std, label=\"std error B, beta2 = 0\", color=\"black\")\n",
    "    axs[0, 0].plot(M, df2_std, label=\"std error B beta2 = 0.08\", color=\"yellow\")\n",
    "    axs[0, 0].plot(M, df3_std, label=\"std error B, beta2 = 0.16\", color=\"red\")\n",
    "    axs[0, 0].set_title(\"Comparaison des valeurs de déviations standard selon n_test\")\n",
    "    axs[0, 0].grid()\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Deuxième subplot : Quartiles de l'erreur selon le modèle\n",
    "    axs[0, 1].plot(M, df1_q1, label=\"Q1 error B, beta = 0\")\n",
    "    axs[0, 1].plot(M, df2_q1, label=\"Q1 error B, beta = 0.08\")\n",
    "    axs[0, 1].plot(M, df3_q1, label=\"Q1 error B, beta = 1\")\n",
    "    axs[0, 1].plot(M, df1_med, label=\"median error B, beta = 0\")\n",
    "    axs[0, 1].plot(M, df2_med, label=\"median error B, beta = 0.08\")\n",
    "    axs[0, 1].plot(M, df3_med, label=\"median error B, beta = 1\")\n",
    "    axs[0, 1].plot(M, df1_q3, label=\"Q3 error B, beta = 0\")\n",
    "    axs[0, 1].plot(M, df2_q3, label=\"Q3 error B, beta = 0.08\")\n",
    "    axs[0, 1].plot(M, df3_q3, label=\"Q3 error B, beta = 1\")\n",
    "    axs[0, 1].set_title(\"Quartiles de l'erreur selon le modèle\")\n",
    "    axs[0, 1].grid()\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Troisième subplot : Quartiles Q1 de l'erreur selon le modèle\n",
    "    axs[1, 0].plot(M, df1_q1, label=\"Q1 error B, beta = 0\")\n",
    "    axs[1, 0].plot(M, df2_q1, label=\"Q1 error B, beta = 0.08\")\n",
    "    axs[1, 0].plot(M, df3_q1, label=\"Q1 error B, beta = 1\")\n",
    "    axs[1, 0].set_title(\"Quartiles Q1 de l'erreur selon beta2\")\n",
    "    axs[1, 0].grid()\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # Quatrième subplot : Quartiles Q3 de l'erreur selon le modèle\n",
    "    axs[1, 1].plot(M, df1_q3, label=\"Q3 error B, beta = 0\")\n",
    "    axs[1, 1].plot(M, df2_q3, label=\"Q3 error B, beta = 0.08\")\n",
    "    axs[1, 1].plot(M, df3_q3, label=\"Q3 error B, beta = 1\")\n",
    "    axs[1, 1].set_title(\"Quartiles Q3 de l'erreur selon le modèle\")\n",
    "    axs[1, 1].grid()\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Inferential statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the standard error of the mean (according to m and beta2), that gives us a first information of the precisation of mA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive formulas (or algorithms) for the standard error (SE) of the mean\n",
    "\n",
    "Methods: \n",
    "- parametric estimates (consider different cases when n is small versus large and when VA is assumed to be Gaussian or not)\n",
    "- bootstrap estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1 - Parametric estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV..A - When n_test is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation paramétrique de l'erreur standard\n",
    "\n",
    "def divide_by_sqrt(df):\n",
    "    return df.apply(lambda x: x / np.sqrt(x.name), axis=0)\n",
    "\n",
    "def standard_error_m_large():\n",
    "\n",
    "    (stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \n",
    "    stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B) = create_data()\n",
    "\n",
    "    df_standard_error_A = divide_by_sqrt(df_empirical_standard_deviations_A)\n",
    "    df_standard_error_B = divide_by_sqrt(df_empirical_standard_deviations_B)\n",
    "\n",
    "    return df_standard_error_A, df_standard_error_B\n",
    "\n",
    "\n",
    "df_standard_error_A, df_standard_error_B = standard_error_m_large()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.B - When n_test is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.C - When n_test is small and not gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2 -  Non parametric estimates / Bootstrap estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2.A - Bootstrap on test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error_bootstrap_test():\n",
    "   \n",
    "   n = 150\n",
    "   np.random.seed(42)\n",
    "\n",
    "   M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "   B = 250\n",
    "\n",
    "   se_bootstrap_A = []\n",
    "   se_bootstrap_B = []\n",
    "\n",
    "   for m in M:\n",
    "   \n",
    "      liste_A = []\n",
    "      liste_B = []\n",
    "      beta1 = 2\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      epsilon_train = epsilon[:n]\n",
    "      x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "      bootstrap_indices = np.random.choice(m, size=(B, m), replace=True)\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "                \n",
    "            liste_boot_A = []\n",
    "            liste_boot_B = []\n",
    "            y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          \n",
    "            for bootstrap_index in bootstrap_indices:\n",
    "         \n",
    "               x_test = x[bootstrap_index]\n",
    "               y_test = beta1 * x_test + beta2 * x_test**2 + epsilon[bootstrap_index]\n",
    "            \n",
    "               model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "               y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "               pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "               \n",
    "               x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "               model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "               y_pred_b = model_b.predict(x_test_quad)\n",
    "               pb = (y_test - y_pred_b)**2\n",
    "\n",
    "               sea = np.std(pa, ddof=1) / (m**0.5)\n",
    "               seb = np.std(pb, ddof = 1) / (m**0.5)\n",
    "\n",
    "               liste_boot_A.append(sea)\n",
    "               liste_boot_B.append(seb)\n",
    "\n",
    "            liste_A.append(np.mean(liste_boot_A))\n",
    "            liste_B.append(np.mean(liste_boot_B))\n",
    "    \n",
    "      se_bootstrap_A.append(liste_A)\n",
    "      se_bootstrap_B.append(liste_B)\n",
    "\n",
    "   df_se_bootstrap_A = pd.DataFrame(se_bootstrap_A, index=M, columns=beta2_values).transpose()\n",
    "   df_se_bootstrap_B = pd.DataFrame(se_bootstrap_B, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_se_bootstrap_A, df_se_bootstrap_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2.B - Subsets of training sets (varying k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error_varying_test_derived():\n",
    "\n",
    "    liste_subsets = [150,200,500,1000,2000]\n",
    "\n",
    "    n = 150\n",
    "    m = 2000\n",
    "    standard_errors_A = []\n",
    "    standard_errors_B = []\n",
    "    np.random.seed(42)\n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    for k in liste_subsets:\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+k]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+k]\n",
    "\n",
    "      liste_A = []\n",
    "      liste_B = []\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "        # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "        # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          se_a = np.std(pa, ddof=1)/(k**0.5)\n",
    "          se_b = np.std(pb, ddof = 1)/(k**0.5)\n",
    "\n",
    "          liste_A.append(se_a)\n",
    "          liste_B.append(se_b)\n",
    "\n",
    "      standard_errors_A.append(liste_A)\n",
    "      standard_errors_B.append(liste_B)\n",
    "\n",
    "    df_se_varying_k_A = pd.DataFrame(standard_errors_A, index=liste_subsets, columns=beta2_values).transpose()\n",
    "    df_se_varying_k_B = pd.DataFrame(standard_errors_B, index=liste_subsets, columns=beta2_values).transpose()\n",
    "\n",
    "    return df_se_varying_k_A, df_se_varying_k_B\n",
    "\n",
    "\n",
    "df_se_varying_k_A, df_se_varying_k_B = standard_error_varying_test_derived()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2.C - Varying k & Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error_varying_bootstrapped_test():\n",
    "   \n",
    "   n = 150\n",
    "   np.random.seed(42)\n",
    "   m = 2000\n",
    "\n",
    "   B = 250\n",
    "\n",
    "   K = [150,200,500,1000,2000]\n",
    "\n",
    "   se_bootstrap_A = []\n",
    "   se_bootstrap_B = []\n",
    "\n",
    "   for k in K:\n",
    "   \n",
    "      liste_A = []\n",
    "      liste_B = []\n",
    "      beta1 = 2\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+k]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+k]\n",
    "\n",
    "      x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "\n",
    "      bootstrap_indices = np.random.choice(k, size=(B, k), replace=True)\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "                \n",
    "            liste_boot_A = []\n",
    "            liste_boot_B = []\n",
    "            y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          \n",
    "            for bootstrap_index in bootstrap_indices:\n",
    "         \n",
    "               \n",
    "               x_test = x[bootstrap_index]\n",
    "               y_test = beta1 * x_test + beta2 * x_test**2 + epsilon[bootstrap_index]\n",
    "            \n",
    "               model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "               y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "               pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "               \n",
    "               x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "               model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "               y_pred_b = model_b.predict(x_test_quad)\n",
    "               pb = (y_test - y_pred_b)**2\n",
    "\n",
    "               sea = np.std(pa, ddof=1) / (k**0.5)\n",
    "               seb = np.std(pb, ddof = 1) / (k**0.5)\n",
    "\n",
    "               liste_boot_A.append(sea)\n",
    "               liste_boot_B.append(seb)\n",
    "\n",
    "            liste_A.append(np.mean(liste_boot_A))\n",
    "            liste_B.append(np.mean(liste_boot_B))\n",
    "    \n",
    "      se_bootstrap_A.append(liste_A)\n",
    "      se_bootstrap_B.append(liste_B)\n",
    "\n",
    "   df_se_bootstrap_A = pd.DataFrame(se_bootstrap_A, index=K, columns=beta2_values).transpose()\n",
    "   df_se_bootstrap_B = pd.DataFrame(se_bootstrap_B, index=K, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_se_bootstrap_A, df_se_bootstrap_B\n",
    "\n",
    "df_se_varying_bootstrap_A_test, df_se_varying_bootstrap_B_test = standard_error_varying_bootstrapped_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.3 - Comparison between various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.3.A - Comparison between parametric method and bootstrap estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_se_bootstrap_A_test, df_se_bootstrap_B_test = standard_error_bootstrap_test() \n",
    "df_standard_error_A, df_standard_error_B = standard_error_m_large()\n",
    "\n",
    "# Première figure : Comparaison des valeurs d'erreur standard obtenues selon la méthode\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax1.plot(beta2_values, df_standard_error_A[2000], label=\"Méthode paramétrique\", color=\"blue\")\n",
    "ax1.plot(beta2_values, df_se_bootstrap_A_test[2000], label=\"Méthode bootstrap sur ensemble de test\", color=\"red\")\n",
    "ax1.set_title(\"Erreur standard pour A selon la méthode\")\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "# Sauvegarde ou affichage de la première figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Deuxième figure : Comparaison des valeurs d'erreur standard selon le bootstrapping\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
    "ax2.plot(beta2_values, df_standard_error_A[2000], label=\"Méthode paramétrique A\", color=\"blue\")\n",
    "ax2.plot(beta2_values, df_se_bootstrap_A_test[2000], label=\"Méthode bootstrap A sur ensemble de test\", color=\"red\")\n",
    "ax2.plot(beta2_values, df_standard_error_B[2000], label=\"Méthode paramétrique B\", color=\"darkblue\")\n",
    "ax2.plot(beta2_values, df_se_bootstrap_B_test[2000], label=\"Méthode bootstrap B sur ensemble de test\", color=\"darkred\")\n",
    "ax2.set_xlabel(\"beta2\")\n",
    "ax2.set_ylabel(\"standard error\")\n",
    "ax2.set_title(\"Erreur standard pour A et B\")\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "# Sauvegarde ou affichage de la deuxième figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "beta2_1 = 0.0\n",
    "beta2_2 = 0.08\n",
    "beta2_3 = 0.16\n",
    "\n",
    "df1 = df_standard_error_A.loc[beta2_1]\n",
    "df2 = df_standard_error_A.loc[beta2_2]\n",
    "df3 = df_standard_error_A.loc[beta2_3]\n",
    "df4 = df_se_bootstrap_A_test.loc[beta2_1]\n",
    "df5 = df_se_bootstrap_A_test.loc[beta2_2]\n",
    "df6 = df_se_bootstrap_A_test.loc[beta2_3]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df1.index, df1.values, marker='o', label = \"erreur standard parametrique selon n_test pour beta2 = \"  f'{beta2_1}', color = \"lightblue\")\n",
    "plt.plot(df4.index, df4.values, marker='o', label = \"erreur standard bootstrap selon n_test pour beta2 = \" f'{beta2_1}', color = \"darkblue\")\n",
    "plt.plot(df2.index, df2.values, marker='o', label = \"erreur standard parametrique selon n_test pour beta2 = \" f'{beta2_2}', color = \"red\")\n",
    "plt.plot(df5.index, df5.values, marker='o', label = \"erreur standard bootstrap selon n_test pour beta2 = \" f'{beta2_2}', color = \"darkred\")\n",
    "plt.plot(df3.index, df3.values, marker='o', label = \"erreur standard parametrique selon n_test pour beta2 = \" f'{beta2_3}', color = \"lightgreen\")\n",
    "plt.plot(df6.index, df6.values, marker='o', label = \"erreur standard bootstrap selon n_test pour beta2 = \" f'{beta2_3}', color = \"darkgreen\")\n",
    "plt.xlabel(\"n_test\")\n",
    "plt.title(f'erreur standard du modèle linéaire pour beta2 = {beta2_1}, {beta2_2}, {beta2_3}')\n",
    "plt.ylabel('Erreur standard')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.3.B - For varying k, comparison between bootstrapped method and parametric method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_se_varying_bootstrap_A_test.loc[beta2_1]\n",
    "df2 = df_se_varying_bootstrap_A_test.loc[beta2_2]\n",
    "df3 = df_se_varying_bootstrap_A_test.loc[beta2_3]\n",
    "df4 = df_se_varying_k_A.loc[beta2_1]\n",
    "df5 = df_se_varying_k_A.loc[beta2_2]\n",
    "df6 = df_se_varying_k_A.loc[beta2_3]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df1.index, df1.values, marker='o', label = \"Bootstrap pour beta2 = \"  f'{beta2_1}', color = \"darkblue\")\n",
    "plt.plot(df1.index, df4.values, marker='o', label = \"Derived pour beta2 = \"  f'{beta2_1}', color = \"lightblue\")\n",
    "plt.plot(df2.index, df2.values, marker='o', label = \"Bootstrap pour beta2 = \" f'{beta2_2}', color = \"darkred\")\n",
    "plt.plot(df2.index, df5.values, marker='o', label = \"Derived pour beta2 = \" f'{beta2_2}', color = \"#FF6666\")\n",
    "plt.plot(df3.index, df3.values, marker='o', label = \"Bootstrap pour beta2 = \" f'{beta2_3}', color = \"darkgreen\")\n",
    "plt.plot(df3.index, df6.values, marker='o', label = \"Derived pour beta2 = \" f'{beta2_3}', color = \"lightgreen\")\n",
    "plt.title('Evolution de l\\'erreur standard du modèle varying k bootstrapped')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Erreur standard')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.3.C - Comparison between varying test samples (k < m) and varying test size (k =m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Comparison of varying method vs reducing test size\")\n",
    "plt.plot(beta2_values, df_se_bootstrap_A_test[1000], label = \"m = 250, smaller size test sample\")\n",
    "plt.plot(beta2_values, df_se_varying_bootstrap_A_test[1000], label = \"m = 2000, k = 250, k < m method\")\n",
    "plt.plot()\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison for a given value \n",
    "df4 = df_se_varying_k_A.loc[beta2_1]\n",
    "df5 = df_se_varying_k_A.loc[beta2_2]\n",
    "df6 = df_se_varying_k_A.loc[beta2_3]\n",
    "dff1 = df_standard_error_A.loc[beta2_1]\n",
    "dff2 = df_standard_error_A.loc[beta2_2]\n",
    "dff3 = df_standard_error_A.loc[beta2_3]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Comparison of standard errors derived method\")\n",
    "\n",
    "\n",
    "plt.plot(df4.index, df4.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_1}', color = \"darkblue\") # confondue\n",
    "plt.plot(dff1.index, dff1.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_1}', color = \"lightblue\")\n",
    "plt.plot(df5.index, df5.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_2}', color = \"darkred\")\n",
    "plt.plot(dff2.index, dff2.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_2}', color = \"red\")\n",
    "plt.plot(df6.index, df6.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_3}', color = \"darkgreen\")\n",
    "plt.plot(dff3.index, dff3.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_3}', color = \"lightgreen\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison for a given value \n",
    "df1 = df_se_varying_bootstrap_A_test.loc[beta2_1]\n",
    "dff1 = df_se_bootstrap_A_test.loc[beta2_1]\n",
    "df2 = df_se_varying_bootstrap_A_test.loc[beta2_2]\n",
    "dff2 = df_se_bootstrap_A_test.loc[beta2_2]\n",
    "df3 = df_se_varying_bootstrap_A_test.loc[beta2_3]\n",
    "dff3 = df_se_bootstrap_A_test.loc[beta2_3]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Comparison of standard errors bootstrap test\")\n",
    "\n",
    "\n",
    "plt.plot(df1.index, df1.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_1}', color = \"darkblue\") # confondue\n",
    "plt.plot(dff1.index, dff1.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_1}', color = \"blue\")\n",
    "plt.plot(df2.index, df2.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_2}', color = \"darkred\")\n",
    "plt.plot(dff2.index, dff2.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_2}', color = \"red\")\n",
    "plt.plot(df3.index, df3.values, marker='o', label = \"Varying method k < m pour beta2 = \" f'{beta2_3}', color = \"darkgreen\")\n",
    "plt.plot(dff3.index, dff3.values, marker='o', label = \"m = k pour beta2 = \" f'{beta2_3}', color = \"lightgreen\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V - Confidence interval of the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1 - Parametric method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interval(lower, upper):\n",
    "      return pd.Interval(left=lower, right=upper, closed='both')\n",
    "\n",
    "\n",
    "def confidence_interval_mean():\n",
    "     \n",
    "     \n",
    "   (stats_A_all, df_empirical_means_A, df_empirical_standard_deviations_A, df_medians_A, df_Q1_A, df_Q3_A, df_IQR_A, \n",
    "     stats_B_all, df_empirical_means_B, df_empirical_standard_deviations_B, df_medians_B, df_Q1_B, df_Q3_B, df_IQR_B) = create_data()\n",
    "    \n",
    "   df_standard_error_A, df_standard_error_B = standard_error_m_large()\n",
    "\n",
    "   t_critical = 1.96\n",
    "   CI_lower_A = df_empirical_means_A - t_critical * df_standard_error_A\n",
    "   CI_lower_B = df_empirical_means_B - t_critical * df_standard_error_B\n",
    "   CI_upper_A = df_empirical_means_A + t_critical * df_standard_error_A\n",
    "   CI_upper_B = df_empirical_means_B + t_critical * df_standard_error_B\n",
    "     \n",
    "   confidence_intervals_A = pd.DataFrame(index=CI_lower_A.index, columns=CI_lower_A.columns)\n",
    "   confidence_intervals_B = pd.DataFrame(index=CI_lower_B.index, columns=CI_lower_B.columns)\n",
    "\n",
    "   for col in CI_lower_A.columns:\n",
    "    confidence_intervals_A[col] = CI_lower_A[col].combine(CI_upper_A[col], create_interval)\n",
    "\n",
    "   for col in CI_lower_B.columns:\n",
    "    confidence_intervals_B[col] = CI_lower_B[col].combine(CI_upper_B[col], create_interval)\n",
    "\n",
    "   return confidence_intervals_A, confidence_intervals_B\n",
    "\n",
    "confidence_intervals_A, confidence_intervals_B = confidence_interval_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2 - Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_each_bootstrap(n, B, m):\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ci_bootstrap_A = []\n",
    "    ci_bootstrap_B = []\n",
    "\n",
    "    beta1 = 2\n",
    "    B = 250\n",
    "    alpha = 0.05\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    bootstrap_indices = np.random.choice(m, size=(B, m), replace=True) # bootstrapping\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        moy_boot_A = []\n",
    "        moy_boot_B = []\n",
    "        y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "\n",
    "        for bootstrap_index in bootstrap_indices:\n",
    "       \n",
    "           x_test = x[bootstrap_index]\n",
    "           y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "           y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "           pa = (y_test - y_pred_a)**2\n",
    "\n",
    "           x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "           x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "           model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "           y_pred_b = model_b.predict(x_test_quad)\n",
    "           pb = (y_test - y_pred_b)**2\n",
    "    \n",
    "           moy_boot_A.append(np.mean(pa))\n",
    "           moy_boot_B.append(np.mean(pb))\n",
    "\n",
    "        lower_A = np.percentile(moy_boot_A, 100 * (alpha / 2))\n",
    "        upper_A = np.percentile(moy_boot_A, 100 * (1 - alpha / 2))\n",
    "        lower_B = np.percentile(moy_boot_B, 100 * (alpha / 2))\n",
    "        upper_B = np.percentile(moy_boot_B, 100 * (1 - alpha / 2))\n",
    "\n",
    "        ci_bootstrap_A.append(pd.Interval(left=lower_A, right=upper_A, closed='both'))\n",
    "        ci_bootstrap_B.append(pd.Interval(left=lower_B, right=upper_B, closed='both'))\n",
    "    \n",
    "    return ci_bootstrap_A, ci_bootstrap_B     \n",
    "\n",
    "def confidence_interval_bootstrap():\n",
    "\n",
    "   M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "   liste_ci_bootstrap_A = []\n",
    "   liste_ci_bootstrap_B = []\n",
    "\n",
    "   for m in M:\n",
    "\n",
    "      ci_bootstrap_A, ci_bootstrap_B = conf_each_bootstrap(150,250,m)\n",
    "\n",
    "      liste_ci_bootstrap_A.append(ci_bootstrap_A)\n",
    "      liste_ci_bootstrap_B.append(ci_bootstrap_B)\n",
    "\n",
    "   df_ci_bootstrap_A = pd.DataFrame(liste_ci_bootstrap_A, index=M, columns=beta2_values).transpose()\n",
    "   df_ci_bootstrap_B = pd.DataFrame(liste_ci_bootstrap_B, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_ci_bootstrap_A, df_ci_bootstrap_B\n",
    "\n",
    "df_ci_bootstrap_A, df_ci_bootstrap_B = confidence_interval_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_confidence_intervals_A():\n",
    "\n",
    "    confidence_intervals_A, confidence_intervals_B = confidence_interval_mean()\n",
    "    df_ci_bootstrap_A, df_ci_bootstrap_B = confidence_interval_bootstrap()\n",
    "    df1 = df_ci_bootstrap_A.loc[beta2_1]\n",
    "    df2 = df_ci_bootstrap_A.loc[beta2_2]\n",
    "    df3 = df_ci_bootstrap_A.loc[beta2_3]\n",
    "    df4 = confidence_intervals_A.loc[beta2_1]\n",
    "    df5 = confidence_intervals_A.loc[beta2_2]\n",
    "    df6 = confidence_intervals_A.loc[beta2_3]\n",
    "\n",
    "    y_values = [\n",
    "        [1.053598913427204, 1.068366766251598],\n",
    "        [0.9292457054668812, 0.9833748402327965],\n",
    "        [0.9962042595799346, 0.9983462990635036],\n",
    "        [0.9886493946391038, 0.9893143185309117],\n",
    "        [0.9962861220803533, 1.0083750605773054]\n",
    "    ]\n",
    "    table_1 = [\n",
    "        [1.0390093295288756, 1.1466015653801718],\n",
    "        [0.9361297529233542, 1.0229671747404647],\n",
    "        [0.9962322602731206, 1.0516027736303146],\n",
    "        [0.9993335114928288, 1.0355278335905707],\n",
    "        [1.010542001326182, 1.0375079528724687]\n",
    "    ]\n",
    "    table_2 = [\n",
    "        [1.0688079662229757, 1.2833752084872891],\n",
    "        [0.9769788312057663, 1.1388984494706547],\n",
    "        [1.0437656863998515, 1.1563363689670394],\n",
    "        [1.055111355556954, 1.1263943592363688],\n",
    "        [1.0669256435790109, 1.116089657564828]\n",
    "    ]\n",
    "    table_3 = [\n",
    "        [0.7664312269412913, 1.369792424998975],\n",
    "        [0.7155858600787521, 1.176581810309871],\n",
    "        [0.865445418757162, 1.1309658094405943],\n",
    "        [0.8999263681837879, 1.0781280797392638],\n",
    "        [0.9376255326483326, 1.0652662396334127]\n",
    "    ]\n",
    "    table_4 = [\n",
    "        [0.7877581893125734, 1.3685939092694972],\n",
    "        [0.7190325401994103, 1.178493327109527],\n",
    "        [0.8743764346979922, 1.1442302502484871],\n",
    "        [0.9386904011625568, 1.126800882686477],\n",
    "        [0.9554520642623406, 1.0863686009438385]\n",
    "    ]\n",
    "    table_5 = [\n",
    "        [0.8458032216182627, 1.4184482427628125],\n",
    "        [0.7543543857512403, 1.2265836665003071],\n",
    "        [0.9273727708382743, 1.2132884768753862],\n",
    "        [1.0193129286747022, 1.2254957368259625],\n",
    "        [1.015251783409371, 1.1554206717907454]\n",
    "    ]\n",
    "\n",
    "    x = [150, 200, 500, 1000, 2000]\n",
    "    y1, y2 = zip(*y_values)\n",
    "    y3, y4 = zip(*table_1)\n",
    "    y5, y6 = zip(*table_2)\n",
    "    y7, y8 = zip(*table_3)\n",
    "    y9, y10 = zip(*table_4)\n",
    "    y11, y12 = zip(*table_5)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y1, label='lower bound beta2 = 0', marker='o', color=\"lightblue\")\n",
    "    plt.plot(x, y2, label='upper bound beta2 = 0', marker='o', color=\"darkblue\")\n",
    "    plt.plot(x, y3, label='lower bound beta2 = 0.06', marker='o', color=\"brown\")\n",
    "    plt.plot(x, y4, label='upper bound beta2 = 0.08', marker='o', color=\"darkred\")\n",
    "    plt.plot(x, y5, label='lower bound beta2 = 0.16', marker='o', color=\"lightgreen\")\n",
    "    plt.plot(x, y6, label='upper bound beta2 = 0.16', marker='o', color=\"darkgreen\")\n",
    "\n",
    "    plt.fill_between(x, y1, y2, color='blue', alpha=0.3, label='Interval for beta2 = 0')\n",
    "    plt.fill_between(x, y3, y4, color='red', alpha=0.3, label='Interval for beta2 = 0.08')\n",
    "    plt.fill_between(x, y5, y6, color='yellow', alpha=0.3, label='Interval for beta2 = 0.16')\n",
    "    plt.xlabel('Values of n_test')\n",
    "    plt.ylabel('Interval confidence of the mean A')\n",
    "    plt.title('Boostrapped confidence intervals of mean A as a function of N_test')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y7, label='lower bound beta2 = 0', color=\"lightblue\")\n",
    "    plt.plot(x, y8, label='upper bound beta2 = 0', color=\"darkblue\")\n",
    "    plt.plot(x, y9, label='lower bound beta2 = 0.06', color=\"brown\")\n",
    "    plt.plot(x, y10, label='upper bound beta2 = 0.08', color=\"darkred\")\n",
    "    plt.plot(x, y11, label='lower bound beta2 = 0.16', color=\"lightgreen\")\n",
    "    plt.plot(x, y12, label='upper bound beta2 = 0.16', color=\"darkgreen\")\n",
    "\n",
    "    plt.fill_between(x, y7, y8, color='blue', alpha=0.5, label='Interval for beta2 = 0')\n",
    "    plt.fill_between(x, y9, y10, color='red', alpha=0.5, label='Interval for beta2 = 0.08')\n",
    "    plt.fill_between(x, y11, y12, color='yellow', alpha=0.5, label='Interval for beta2 = 0.16')\n",
    "    plt.xlabel('Values of n_test')\n",
    "    plt.ylabel('Interval confidence of the mean of A')\n",
    "    plt.title('Derived confidence intervals of mean A as a function of N_test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "graphs_confidence_intervals_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_confidence_intervals_B():\n",
    "    confidence_intervals_A, confidence_intervals_B = confidence_interval_mean()\n",
    "    df_ci_bootstrap_A, df_ci_bootstrap_B = confidence_interval_bootstrap()\n",
    "    \n",
    "    df1 = df_ci_bootstrap_B.loc[beta2_1]\n",
    "    df2 = df_ci_bootstrap_B.loc[beta2_2]\n",
    "    df3 = df_ci_bootstrap_B.loc[beta2_3]\n",
    "    df4 = confidence_intervals_B.loc[beta2_1]\n",
    "    df5 = confidence_intervals_B.loc[beta2_2]\n",
    "    df6 = confidence_intervals_B.loc[beta2_3]\n",
    "\n",
    "    y_values = [\n",
    "    [1.053808267884721, 1.0688762122899125],\n",
    "    [0.9292976160288866, 1.0083354785602343],\n",
    "    [0.9937484365303293, 1.0004365255248169],\n",
    "    [0.9857804263439052, 0.9914301642805049],\n",
    "    [1.01907270447993, 1.0464838908607992]\n",
    "]\n",
    "\n",
    "    table_1 = [\n",
    "    [1.0531808267884717, 1.0688762122898913],\n",
    "    [0.9292976160288865, 1.0083354785602343],\n",
    "    [0.9937484365303297, 1.000436525524817],\n",
    "    [0.9857804263439051, 0.9914301642805046],\n",
    "    [1.019072704479927, 1.046483890860799]\n",
    "]\n",
    "\n",
    "    table_2 = [\n",
    "    [1.0531808267884717, 1.0688762122899127],\n",
    "    [0.929297616028887, 1.008335478560235],\n",
    "    [0.9937484365303295, 1.0004365255248169],\n",
    "    [0.985780426343905, 0.9914301642805047],\n",
    "    [1.01907270447993, 1.0464838908607992]\n",
    "]\n",
    "\n",
    "    table_3 = [\n",
    "    [0.7666644036644075, 1.368281244228714],\n",
    "    [0.7352823535856203, 1.2069875315994218],\n",
    "    [0.8670675727868207, 1.132846997115552],\n",
    "    [0.897680106912762, 1.0750969043388854],\n",
    "    [0.9672562473475386, 1.0983035624868063]\n",
    "]\n",
    "\n",
    "    table_4 = [\n",
    "    [0.7666644036644075, 1.3682812442287144],\n",
    "    [0.7352823535856203, 1.2069875315994216],\n",
    "    [0.8670675727868219, 1.132846997115563],\n",
    "    [0.8976801069127621, 1.0750969043388854],\n",
    "    [0.9672562473475383, 1.098303562486806]\n",
    "]\n",
    "\n",
    "    table_5 = [\n",
    "    [0.7666644036644047, 1.3682812442287136],\n",
    "    [0.7352823535856208, 1.2069875315994225],\n",
    "    [0.8670675727868209, 1.132846997115554],\n",
    "    [0.8976801069127622, 1.0750969043388854],\n",
    "    [0.9672562473475388, 1.0983035624868065]\n",
    "]\n",
    "\n",
    "    x = [150, 200, 500, 1000, 2000]\n",
    "    y1, y2 = zip(*y_values)\n",
    "    y3, y4 = zip(*table_1)\n",
    "    y5, y6 = zip(*table_2)\n",
    "    y7, y8 = zip(*table_3)\n",
    "    y9, y10 = zip(*table_4)\n",
    "    y11, y12 = zip(*table_5)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y1, label='lower bound beta2 = 0', marker='o', color=\"lightblue\")\n",
    "    plt.plot(x, y2, label='upper bound beta2 = 0', marker='o', color=\"darkblue\")\n",
    "    plt.plot(x, y3, label='lower bound beta2 = 0.06', marker='o', color=\"brown\")\n",
    "    plt.plot(x, y4, label='upper bound beta2 = 0.08', marker='o', color=\"darkred\")\n",
    "    plt.plot(x, y5, label='lower bound beta2 = 0.16', marker='o', color=\"lightgreen\")\n",
    "    plt.plot(x, y6, label='upper bound beta2 = 0.16', marker='o', color=\"darkgreen\")\n",
    "\n",
    "    plt.fill_between(x, y1, y2, color='blue', alpha=0.3, label='Interval for beta2 = 0')\n",
    "    plt.fill_between(x, y3, y4, color='red', alpha=0.3, label='Interval for beta2 = 0.08')\n",
    "    plt.fill_between(x, y5, y6, color='yellow', alpha=0.3, label='Interval for beta2 = 0.16')\n",
    "    plt.xlabel('Values of n_test')\n",
    "    plt.ylabel('Interval confidence of the mean for B')\n",
    "    plt.title('Boostrapped confidence intervals of mean B as a function of N_test')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y7, label='lower bound beta2 = 0', color=\"lightblue\")\n",
    "    plt.plot(x, y8, label='upper bound beta2 = 0', color=\"darkblue\")\n",
    "    plt.plot(x, y9, label='lower bound beta2 = 0.06', color=\"brown\")\n",
    "    plt.plot(x, y10, label='upper bound beta2 = 0.08', color=\"darkred\")\n",
    "    plt.plot(x, y11, label='lower bound beta2 = 0.16', color=\"lightgreen\")\n",
    "    plt.plot(x, y12, label='upper bound beta2 = 0.16', color=\"darkgreen\")\n",
    "\n",
    "    plt.fill_between(x, y7, y8, color='blue', alpha=0.5, label='Interval for beta2 = 0')\n",
    "    plt.fill_between(x, y9, y10, color='red', alpha=0.5, label='Interval for beta2 = 0.08')\n",
    "    plt.fill_between(x, y11, y12, color='yellow', alpha=0.5, label='Interval for beta2 = 0.16')\n",
    "    plt.xlabel('Values of n_test')\n",
    "    plt.ylabel('Interval confidence of the mean for B')\n",
    "    plt.title('Derived confidence intervals of mean B as a function of N_test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "graphs_confidence_intervals_B()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI - Inferential statistics : is a model better than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1 - Confidence interval for the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1.A - Derived method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics_diff():\n",
    "    \n",
    "    n = 150\n",
    "\n",
    "    M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "    empirical_means_diff = []\n",
    "    standard_deviations_diff = []\n",
    "\n",
    "    for m in M:\n",
    "\n",
    "      np.random.seed(42)\n",
    "      beta1 = 2\n",
    "      beta2_values_bis = [0.2, 0.3, 0.4, 0.5, 1]\n",
    "      epsilon = np.random.normal(0, 1, n + m) \n",
    "      x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "      x_train = x[:n]\n",
    "      x_test = x[n:n+m]\n",
    "\n",
    "      epsilon_train = epsilon[:n]\n",
    "      epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "      means = []\n",
    "      stds =[]\n",
    "\n",
    "    \n",
    "      for beta2 in beta2_values:\n",
    "            \n",
    "          y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "          y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "        # Linear Model : A\n",
    "          model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "          y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "          pa = (y_test - y_pred_a)**2\n",
    "    \n",
    "        # Quadratic Model : B\n",
    "          x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "          x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "            \n",
    "          model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "          y_pred_b = model_b.predict(x_test_quad)\n",
    "          pb = (y_test - y_pred_b)**2\n",
    "\n",
    "          differences = pa - pb\n",
    "\n",
    "          mean = sum(differences)/m\n",
    "          std = np.std(differences, ddof=1)\n",
    "          means.append(mean)\n",
    "          stds.append(std)\n",
    "\n",
    "      empirical_means_diff.append(means)\n",
    "      standard_deviations_diff.append(stds)\n",
    "\n",
    "    return empirical_means_diff, standard_deviations_diff\n",
    "\n",
    "def create_data_diff():\n",
    "    \n",
    "    M = [150, 200, 500, 1000, 2000]\n",
    "    \n",
    "    empirical_means_diff, standard_deviations_diff = descriptive_statistics_diff()\n",
    "\n",
    "\n",
    "\n",
    "    df_empirical_means_diff = pd.DataFrame(empirical_means_diff, index=M, columns=beta2_values).transpose()\n",
    "    df_empirical_standard_deviations_diff = pd.DataFrame(standard_deviations_diff, index=M, columns=beta2_values).transpose()\n",
    "    \n",
    "\n",
    "    return df_empirical_means_diff, df_empirical_standard_deviations_diff\n",
    "\n",
    "\n",
    "def divide_by_sqrt(df):\n",
    "    return df.apply(lambda x: x / np.sqrt(x.name), axis=0)\n",
    "\n",
    "def standard_error_m_large_diff():\n",
    "\n",
    "    df_empirical_means_diff, df_empirical_standard_deviations_diff = create_data_diff()\n",
    "\n",
    "    df_standard_error_diff = divide_by_sqrt(df_empirical_standard_deviations_diff)\n",
    "\n",
    "    return df_standard_error_diff\n",
    "\n",
    "\n",
    "df_standard_error_diff = standard_error_m_large_diff()\n",
    "\n",
    "def create_interval(lower, upper):\n",
    "      return pd.Interval(left=lower, right=upper, closed='both')\n",
    "\n",
    "\n",
    "def confidence_interval_mean_diff():\n",
    "     \n",
    "     \n",
    "   df_empirical_means_diff, df_empirical_standard_deviations_diff = create_data_diff()\n",
    "    \n",
    "   df_standard_error_diff = standard_error_m_large_diff()\n",
    "\n",
    "   t_critical = 1.96\n",
    "   CI_lower_diff = df_empirical_means_diff - t_critical * df_standard_error_diff\n",
    "   CI_upper_diff = df_empirical_means_diff + t_critical * df_standard_error_diff\n",
    "\n",
    "     \n",
    "   confidence_intervals_diff = pd.DataFrame(index=CI_lower_diff.index, columns=CI_lower_diff.columns)\n",
    "\n",
    "   for col in CI_lower_diff.columns:\n",
    "    confidence_intervals_diff[col] = CI_lower_diff[col].combine(CI_upper_diff[col], create_interval)\n",
    "\n",
    "\n",
    "   return confidence_intervals_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_confidence_intervals_diff():\n",
    "\n",
    "    confidence_intervals_diff = confidence_interval_mean_diff()\n",
    "  \n",
    "    df4 = confidence_intervals_diff.loc[beta2_1]\n",
    "    df5 = confidence_intervals_diff.loc[beta2_2]\n",
    "    df6 = confidence_intervals_diff.loc[beta2_3]\n",
    "\n",
    "    y_values = [\n",
    "    [-0.00224048499549938, 0.0035020525466965434],\n",
    "    [-0.055586561742080424, 0.005484346945660815],\n",
    "    [-0.0047738783907312725, 0.0012688340901101367],\n",
    "    [-0.000298699052576929, 0.005570306576659783],\n",
    "    [-0.04647783243064141, -0.016190205121959147]\n",
    "]\n",
    "\n",
    "    table_1 = [\n",
    "    [-0.033615345464745254, 0.05502179615369398],\n",
    "    [-0.09216435241715609, 0.04742033454105213],\n",
    "    [-0.0218683137802478, 0.040577243826127075],\n",
    "    [0.02384741082744665, 0.06886686176993852],\n",
    "    [-0.040706890144407032, 0.01697745515906775]\n",
    "]\n",
    "\n",
    "    table_2 = [\n",
    "    [-0.027037504233300133, 0.1564332072125412],\n",
    "    [-0.09032617142688239, 0.1289943384938562],\n",
    "    [0.010822213501137913, 0.1299276714144535],\n",
    "    [0.09286671611121941, 0.17916493813779735],\n",
    "    [0.009693510844370888, 0.09541913452140233]\n",
    "]\n",
    "    \n",
    "\n",
    "    x = [150, 200, 500, 1000, 2000]\n",
    "    y1, y2 = zip(*y_values)\n",
    "    y3, y4 = zip(*table_1)\n",
    "    y5, y6 = zip(*table_2)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y1, label='lower bound beta2 = 0', marker='o', color=\"lightblue\")\n",
    "    plt.plot(x, y2, label='upper bound beta2 = 0', marker='o', color=\"darkblue\")\n",
    "    plt.plot(x, y3, label='lower bound beta2 = 0.06', marker='o', color=\"brown\")\n",
    "    plt.plot(x, y4, label='upper bound beta2 = 0.08', marker='o', color=\"darkred\")\n",
    "    plt.plot(x, y5, label='lower bound beta2 = 0.16', marker='o', color=\"lightgreen\")\n",
    "    plt.plot(x, y6, label='upper bound beta2 = 0.16', marker='o', color=\"darkgreen\")\n",
    "\n",
    "    plt.fill_between(x, y1, y2, color='blue', alpha=0.3, label='Interval for beta2 = 0')\n",
    "    plt.fill_between(x, y3, y4, color='red', alpha=0.3, label='Interval for beta2 = 0.08')\n",
    "    plt.fill_between(x, y5, y6, color='yellow', alpha=0.3, label='Interval for beta2 = 0.16')\n",
    "    plt.xlabel('Values of n_test')\n",
    "    plt.ylabel('Interval confidence of the mean of diff')\n",
    "    plt.title('Derived confidence intervals of mean diff as a function of N_test')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1.B - Bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_each_bootstrap_diff(n, B, m):\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ci_bootstrap_diff = []\n",
    "\n",
    "    beta1 = 2\n",
    "    B = 250\n",
    "    alpha = 0.05\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    bootstrap_indices = np.random.choice(m, size=(B, m), replace=True) # bootstrapping\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        moy_boot = []\n",
    "        y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "\n",
    "        for bootstrap_index in bootstrap_indices:\n",
    "       \n",
    "           x_test = x[bootstrap_index]\n",
    "           y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "           y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "           pa = (y_test - y_pred_a)**2\n",
    "\n",
    "           x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "           x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "           model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "           y_pred_b = model_b.predict(x_test_quad)\n",
    "           pb = (y_test - y_pred_b)**2\n",
    "\n",
    "           difference = pa - pb\n",
    "    \n",
    "           moy_boot.append(np.mean(difference))\n",
    "           \n",
    "\n",
    "        lower_A = np.percentile(moy_boot, 100 * (alpha / 2))\n",
    "        upper_A = np.percentile(moy_boot, 100 * (1 - alpha / 2))\n",
    "\n",
    "\n",
    "        ci_bootstrap_diff.append(pd.Interval(left=lower_A, right=upper_A, closed='both'))\n",
    "    \n",
    "    return ci_bootstrap_diff    \n",
    "\n",
    "def confidence_interval_bootstrap_diff():\n",
    "\n",
    "   M = [150, 200, 500, 1000, 2000]\n",
    "\n",
    "   liste_ci_bootstrap_diff = []\n",
    "\n",
    "   for m in M:\n",
    "\n",
    "      ci_bootstrap_diff = conf_each_bootstrap_diff(150,250,m)\n",
    "\n",
    "      liste_ci_bootstrap_diff.append(ci_bootstrap_diff)\n",
    "\n",
    "   df_ci_bootstrap_diff = pd.DataFrame(liste_ci_bootstrap_diff, index=M, columns=beta2_values).transpose()\n",
    "\n",
    "   return df_ci_bootstrap_diff\n",
    "\n",
    "df_ci_bootstrap_diff = confidence_interval_bootstrap_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_confidence_intervals_boot_diff():\n",
    "\n",
    "    df_ci_bootstrap_diff = confidence_interval_bootstrap_diff()\n",
    "  \n",
    "    df4 = df_ci_bootstrap_diff.loc[beta2_1]\n",
    "    df5 = df_ci_bootstrap_diff.loc[beta2_2]\n",
    "    df6 = df_ci_bootstrap_diff.loc[beta2_3]\n",
    "\n",
    "    y_values = [\n",
    "    [-0.003512964579693098, 0.0028840226022357764],\n",
    "    [-0.04380779140377065, 0.01563652071406074],\n",
    "    [-0.00309317748416536, 0.003051016152389021],\n",
    "    [-0.002632494411487736, 0.003061574986012803],\n",
    "    [-0.042311176847253454, -0.016673972045885124]\n",
    "]\n",
    "\n",
    "    table_1 = [\n",
    "    [-0.020543797503679004, 0.08059332100761392],\n",
    "    [-0.0546462587354289, 0.08022161602502724],\n",
    "    [-0.00426251210462769, 0.05818729143812169],\n",
    "    [0.007948508651204597, 0.049743690822452905],\n",
    "    [-0.031922069955109655, 0.01643259944779306]\n",
    "]\n",
    "\n",
    "    table_2 = [\n",
    "    [0.006554879722131183, 0.21821821194090696],\n",
    "    [-0.01765086059119779, 0.1977468360437992],\n",
    "    [0.04326867542452211, 0.16253827878341376],\n",
    "    [0.06373489985349622, 0.14070408077429012],\n",
    "    [0.02385983683331472, 0.09556055683693368]\n",
    "]\n",
    "\n",
    "    x = [150, 200, 500, 1000, 2000]\n",
    "    y1, y2 = zip(*y_values)\n",
    "    y3, y4 = zip(*table_1)\n",
    "    y5, y6 = zip(*table_2)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y1, label='lower bound beta2 = 0', marker='o', color=\"lightblue\")\n",
    "    plt.plot(x, y2, label='upper bound beta2 = 0', marker='o', color=\"darkblue\")\n",
    "    plt.plot(x, y3, label='lower bound beta2 = 0.06', marker='o', color=\"brown\")\n",
    "    plt.plot(x, y4, label='upper bound beta2 = 0.08', marker='o', color=\"darkred\")\n",
    "    plt.plot(x, y5, label='lower bound beta2 = 0.16', marker='o', color=\"lightgreen\")\n",
    "    plt.plot(x, y6, label='upper bound beta2 = 0.16', marker='o', color=\"darkgreen\")\n",
    "\n",
    "    plt.fill_between(x, y1, y2, color='blue', alpha=0.3, label='Interval for beta2 = 0')\n",
    "    plt.fill_between(x, y3, y4, color='red', alpha=0.3, label='Interval for beta2 = 0.08')\n",
    "    plt.fill_between(x, y5, y6, color='yellow', alpha=0.3, label='Interval for beta2 = 0.16')\n",
    "    plt.xlabel('Values of n_test')\n",
    "    plt.ylabel('Interval confidence of the mean of diff')\n",
    "    plt.title('Bootstrapped confidence intervals of mean diff as a function of N_test')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2 - Statistical testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2.A - Parametric testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_simulation():\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n= 150\n",
    "    m = 2000\n",
    "    power = []\n",
    "    standard_errors = []\n",
    "    test_statistics = []\n",
    "    p_values = []\n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        \n",
    "        y_train = beta1 * x_train + beta2 * x_train **2 + epsilon_train\n",
    "        y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "        # Linear Model\n",
    "        model_a1 = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "        y_pred_a1 = model_a1.predict(x_test.reshape(-1, 1))\n",
    "        pa = (y_test-y_pred_a1)**2\n",
    "\n",
    "            # Quadratic Model\n",
    "        x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "        x_test_quad = np.column_stack((x_test, x_test**2))    \n",
    "        model_a2 = LinearRegression().fit(x_train_quad, y_train)\n",
    "        y_pred_a2 = model_a2.predict(x_test_quad)\n",
    "        pb = (y_test-y_pred_a2)**2\n",
    "\n",
    "        diff = pa - pb\n",
    "\n",
    "        d_barre = np.mean(diff)\n",
    "        se = np.std(diff)/(m**0.5)\n",
    " \n",
    "        test_stat = d_barre/se # Statistique pour l'hypothèse nulle\n",
    "\n",
    "        student = stats.t.ppf(0.95, df=m-1)\n",
    "        p_value = 1 - stats.t.cdf(abs(test_stat), df = m-1)\n",
    "\n",
    "        test_alt = student - test_stat\n",
    "        puissance = 1 - stats.norm.cdf(test_alt, 0, 1)\n",
    "       \n",
    "        standard_errors.append(se)\n",
    "        test_statistics.append(test_stat)\n",
    "        p_values.append(p_value)\n",
    "        power.append(puissance)\n",
    "\n",
    "    return  standard_errors, test_statistics, p_values, power\n",
    "\n",
    "\n",
    "standard_errors, test_statistics, p_values, power = power_simulation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2.B - Bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_simulation_boot():\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n = 150\n",
    "    m = 2000\n",
    "    power = []\n",
    "    standard_errors = []\n",
    "    test_statistics = []\n",
    "    p_values = []\n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    B = 250\n",
    "    bootstrap_indices = np.random.choice(m, size=(B, m), replace=True) # bootstrapping\n",
    "    \n",
    "    for beta2 in beta2_values:\n",
    "\n",
    "        moy_boot_diff = []\n",
    "        y_train = beta1 * x_train + beta2 * x_train**2 + epsilon_train\n",
    "\n",
    "        for bootstrap_index in bootstrap_indices:\n",
    "       \n",
    "           x_test = x[bootstrap_index]\n",
    "           y_test = beta1 * x_test + beta2 * x_test**2 + epsilon_test\n",
    "\n",
    "           model_a = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "           y_pred_a = model_a.predict(x_test.reshape(-1, 1))\n",
    "           pa = (y_test - y_pred_a)**2\n",
    "\n",
    "           x_train_quad = np.column_stack((x_train, x_train**2))\n",
    "           x_test_quad = np.column_stack((x_test, x_test**2))\n",
    "\n",
    "           model_b = LinearRegression().fit(x_train_quad, y_train)\n",
    "           y_pred_b = model_b.predict(x_test_quad)\n",
    "           pb = (y_test - y_pred_b)**2\n",
    "\n",
    "           diff = pa - pb\n",
    "    \n",
    "           moy_boot_diff.append(np.mean(diff))\n",
    "\n",
    "        d_barre = np.mean(moy_boot_diff)\n",
    "        se = np.std(moy_boot_diff)\n",
    " \n",
    "        test_stat = d_barre/se # Statistique pour l'hypothèse nulle\n",
    "\n",
    "        student = stats.t.ppf(0.95, df=m-1)\n",
    "        p_value = 1 - stats.t.cdf(abs(test_stat), df = m-1)\n",
    "        \n",
    "        test_alt = student - test_stat\n",
    "        puissance = 1 - stats.norm.cdf(test_alt, 0, 1)\n",
    "       \n",
    "        standard_errors.append(se)\n",
    "        test_statistics.append(test_stat)\n",
    "        power.append(puissance)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    return  standard_errors, test_statistics, p_values, power\n",
    "\n",
    "\n",
    "standard_errors_boot, test_statistics_boot, p_values_boot, power_boot = power_simulation_boot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard Errors:\", standard_errors_boot)\n",
    "print(\"Test Statistics:\", test_statistics_boot)\n",
    "print(\"Power:\", power_boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"erreur standard du test selon beta2\")\n",
    "plt.plot(beta2_values, standard_errors, label = \"puissance\", color = \"red\")\n",
    "plt.plot(beta2_values, standard_errors_boot, label = \"boot\", color = \"b\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"P valeurs du test selon beta2\")\n",
    "plt.plot(beta2_values, p_values_boot, label = \"boot\", color = \"b\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Statistique de test du test selon beta2\")\n",
    "plt.plot(beta2_values, test_statistics, label = \"puissance\", color = \"red\")\n",
    "plt.plot(beta2_values, test_statistics_boot, label = \"boot\", color = \"b\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Puissance du test selon beta2\")\n",
    "plt.plot(beta2_values, power_boot, label = \"puissance bootstrap\", color = \"darkred\")\n",
    "plt.plot(beta2_values, power, label = \"puissance paramétrique\", color = \"red\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sépare génération // notebook qui fit et sort les erreurs (tsv) // notebook analyse stat / ranger celui là \n",
    "\n",
    "wiki bootstrap pour intervalle de confiance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2.C - Permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hypothèse nulle $H_0$ et l'hypothèse alternative $H_1$ sont définies comme suit :\n",
    "\\begin{align*}\n",
    "H_0 &: \\mu_A < \\mu_B \\\\\n",
    "H_a &: \\mu_A > \\mu_B\n",
    "\\end{align*}\n",
    "\n",
    "La statistique de test se calcule comme le rapport de la moyenne empirique de la différence D = A - B, $m_D$ divisée par l'erreur standard de la moyenne de D, $SE(Dbarre)$ :\n",
    "$$\n",
    "T = \\frac{m_D}{SE(Dbarre)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_simulation_with_permutation(n_permutations=1000):\n",
    "    np.random.seed(42)\n",
    "    n = 150\n",
    "    m = 2000\n",
    "    power = []\n",
    "    standard_errors = []\n",
    "    test_statistics = []\n",
    "    permutation_p_values = []\n",
    "    \n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    for beta2 in beta2_values:\n",
    "        y_train = beta1 * x_train + beta2 * x_train ** 2 + epsilon_train\n",
    "        y_test = beta1 * x_test + beta2 * x_test ** 2 + epsilon_test\n",
    "\n",
    "        # Linear Model\n",
    "        model_a1 = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "        y_pred_a1 = model_a1.predict(x_test.reshape(-1, 1))\n",
    "        pa = (y_test - y_pred_a1) ** 2\n",
    "\n",
    "        # Quadratic Model\n",
    "        x_train_quad = np.column_stack((x_train, x_train ** 2))\n",
    "        x_test_quad = np.column_stack((x_test, x_test ** 2))    \n",
    "        model_a2 = LinearRegression().fit(x_train_quad, y_train)\n",
    "        y_pred_a2 = model_a2.predict(x_test_quad)\n",
    "        pb = (y_test - y_pred_a2) ** 2\n",
    "\n",
    "        diff = pa - pb\n",
    "        d_barre = np.mean(diff)\n",
    "        se = np.std(diff) / (m ** 0.5)\n",
    "        test_stat = d_barre / se\n",
    "\n",
    "        perm_stats = []\n",
    "        for _ in range(n_permutations):\n",
    "            perm_indices = np.random.permutation(m)\n",
    "            perm_diff = pa[perm_indices] - pb[perm_indices]\n",
    "            perm_d_barre = np.mean(perm_diff)\n",
    "            perm_stats.append(perm_d_barre / se)\n",
    "\n",
    "        perm_stats = np.array(perm_stats)\n",
    "        p_value = (np.sum(perm_stats >= test_stat) + 1) / (n_permutations + 1)\n",
    "        permutation_p_values.append(p_value)  \n",
    "        puissance =  np.mean(np.array(permutation_p_values) < 0.05)\n",
    "\n",
    "        standard_errors.append(se)\n",
    "        test_statistics.append(test_stat)\n",
    "        power.append(puissance)\n",
    "\n",
    "    return standard_errors, test_statistics, power, permutation_p_values\n",
    "\n",
    "standard_errors_perm, test_statistics_perm, power_perm, permutation_p_values = power_simulation_with_permutation()\n",
    "\n",
    "print(\"Standard Errors:\", standard_errors)\n",
    "print(\"Test Statistics:\", test_statistics)\n",
    "print(\"Power:\", power_perm)\n",
    "print(\"Permutation P-values:\", permutation_p_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_simulation_with_permutation_2(n_permutations=1000):\n",
    "    np.random.seed(42)\n",
    "    n = 150\n",
    "    m = 2000\n",
    "    power = []\n",
    "    power_perm = []\n",
    "    standard_errors = []\n",
    "    test_statistics = []\n",
    "    permutation_p_values = []\n",
    "    \n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    for beta2 in beta2_values:\n",
    "        y_train = beta1 * x_train + beta2 * x_train ** 2 + epsilon_train\n",
    "        y_test = beta1 * x_test + beta2 * x_test ** 2 + epsilon_test\n",
    "\n",
    "        # Linear Model\n",
    "        model_a1 = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "        y_pred_a1 = model_a1.predict(x_test.reshape(-1, 1))\n",
    "        pa = (y_test - y_pred_a1) ** 2\n",
    "\n",
    "        # Quadratic Model\n",
    "        x_train_quad = np.column_stack((x_train, x_train ** 2))\n",
    "        x_test_quad = np.column_stack((x_test, x_test ** 2))    \n",
    "        model_a2 = LinearRegression().fit(x_train_quad, y_train)\n",
    "        y_pred_a2 = model_a2.predict(x_test_quad)\n",
    "        pb = (y_test - y_pred_a2) ** 2\n",
    "\n",
    "        diff = pa - pb\n",
    "        d_barre = np.mean(diff)\n",
    "        se = np.std(diff) / (m ** 0.5)\n",
    "        test_stat = d_barre / se\n",
    "\n",
    "        perm_stats = []\n",
    "        for _ in range(n_permutations):\n",
    "            perm_indices = np.random.permutation(m)\n",
    "            perm_diff = pa[perm_indices] - pb\n",
    "            perm_d_barre = np.mean(perm_diff)\n",
    "            perm_stats.append(perm_d_barre / se)\n",
    "\n",
    "        perm_stats = np.array(perm_stats)\n",
    "\n",
    "        p_value = (np.sum(perm_stats >= test_stat) + 1) / (n_permutations + 1)\n",
    "        permutation_p_values.append(p_value)\n",
    "\n",
    "\n",
    "        perm_student = stats.t.ppf(0.95, df=m-1)\n",
    "        perm_test_alt = perm_student - perm_stats\n",
    "        perm_puissance = 1 - stats.norm.cdf(perm_test_alt, 0, 1)\n",
    "\n",
    "        power_perm.append(np.mean(perm_puissance))\n",
    "\n",
    "        standard_errors.append(se)\n",
    "        test_statistics.append(test_stat)\n",
    "\n",
    "    return standard_errors, test_statistics, power_perm, permutation_p_values\n",
    "\n",
    "standard_errors_perm_2, test_statistics_perm_2, power_perm_2, permutation_p_values_2 = power_simulation_with_permutation_2()\n",
    "\n",
    "print(\"Standard Errors:\", standard_errors_perm_2)\n",
    "print(\"Test Statistics:\", test_statistics_perm_2)\n",
    "print(\"Power (Permutation):\", power_perm_2)\n",
    "print(\"Permutation P-values:\", permutation_p_values_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Puissance du test selon beta2\")\n",
    "plt.plot(beta2_values, power_boot, label = \"puissanc testbootstrap\", color = \"darkred\")\n",
    "plt.plot(beta2_values, power, label = \"puissance test paramétrique\", color = \"red\")\n",
    "plt.plot(beta2_values, power_perm, label = \"puissance test permutation 2\", color = \"black\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2.D - Non parametric testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def power_simulation_with_wilcoxon():\n",
    "    np.random.seed(42)\n",
    "    n = 150\n",
    "    m = 2000\n",
    "    power = []\n",
    "    standard_errors = []\n",
    "    test_statistics = []\n",
    "    wilcoxon_p_values = []\n",
    "    \n",
    "    beta1 = 2\n",
    "    epsilon = np.random.normal(0, 1, n + m) \n",
    "    x = np.random.uniform(0, 5, n + m)\n",
    "\n",
    "    x_train = x[:n]\n",
    "    x_test = x[n:n+m]\n",
    "    epsilon_train = epsilon[:n]\n",
    "    epsilon_test = epsilon[n:n+m]\n",
    "\n",
    "    for beta2 in beta2_values:\n",
    "        y_train = beta1 * x_train + beta2 * x_train ** 2 + epsilon_train\n",
    "        y_test = beta1 * x_test + beta2 * x_test ** 2 + epsilon_test\n",
    "\n",
    "        # Linear Model\n",
    "        model_a1 = LinearRegression().fit(x_train.reshape(-1, 1), y_train)\n",
    "        y_pred_a1 = model_a1.predict(x_test.reshape(-1, 1))\n",
    "        pa = (y_test - y_pred_a1) ** 2\n",
    "\n",
    "        # Quadratic Model\n",
    "        x_train_quad = np.column_stack((x_train, x_train ** 2))\n",
    "        x_test_quad = np.column_stack((x_test, x_test ** 2))    \n",
    "        model_a2 = LinearRegression().fit(x_train_quad, y_train)\n",
    "        y_pred_a2 = model_a2.predict(x_test_quad)\n",
    "        pb = (y_test - y_pred_a2) ** 2\n",
    "\n",
    "        diff = pa - pb\n",
    "        d_barre = np.mean(diff)\n",
    "        se = np.std(diff) / (m ** 0.5)\n",
    "        test_stat = d_barre / se\n",
    "\n",
    "        wilcoxon_stat, p_value = wilcoxon(diff)\n",
    "        wilcoxon_p_values.append(p_value)\n",
    "\n",
    "        puissance = np.mean(np.array(wilcoxon_p_values) < 0.05)\n",
    "        standard_errors.append(se)\n",
    "        test_statistics.append(test_stat)\n",
    "        power.append(puissance)\n",
    "\n",
    "    return standard_errors, test_statistics, power, wilcoxon_p_values\n",
    "\n",
    "standard_errors_wil, test_statistics_wil, power_wil, wilcoxon_p_values = power_simulation_with_wilcoxon()\n",
    "\n",
    "print(\"Standard Errors:\", standard_errors_wil)\n",
    "print(\"Test Statistics:\", test_statistics_wil)\n",
    "print(\"Power:\", power_wil)\n",
    "print(\"Wilcoxon P-values:\", wilcoxon_p_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Puissance du test selon beta2\")\n",
    "plt.plot(beta2_values, power_boot, label = \"puissanc testbootstrap\", color = \"darkred\")\n",
    "plt.plot(beta2_values, power, label = \"puissance test paramétrique\", color = \"red\")\n",
    "plt.plot(beta2_values, power_perm, label = \"puissance test permutation\", color = \"black\")\n",
    "plt.plot(beta2_values, power_wil, label = \"puissance test non param\", color = \"blue\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
